{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4555d86-4eb4-4185-93d8-52f55da43df9",
   "metadata": {},
   "source": [
    "# Filter Training Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f767b5b-dd6e-48f8-aba4-259d72d8804a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Background\n",
    "\n",
    "It is not uncommon that existing training data were collected at different time period than the study period. This means the dataset may not reflect the real ground cover due to temporal changes. FAO adopted a training data filtering method for any given reference year that is within a time span (e.g. 5 years) from an existing baseline, and tested the method in the production of land cover mapping for Lesotho. It is assumed that the majority of reference labels will remain valid from one year to the previous/next. Based on this assumption, the reference labels which have changed are the minority, and should be detectable through the use of outlier detection methods like K-Means clustering. More details on the method and how it works for Lesotho can be found in the published paper ([De Simone et al 2022](https://www.mdpi.com/2072-4292/14/14/3294))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ec80f-8f94-45f9-808c-712a572f8d89",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "This notebook will implement FAO's automatic filtering of a training dataset for a target year using points from a geojson or shapefile and a reference classification map of a previous year. The steps include:\n",
    "1. Load extracted training features\n",
    "2. Generate stratified random samples for each class on the reference land cover map using `random_sampling` and extract their features using `collect_training_data`\n",
    "3. Train K-Means models using the extracted features of the random samples\n",
    "4. Apply clustering on training features and remove minor clusters\n",
    "5. Export the filtered training features to disk for use in subsequent scripts\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdba3177-6db9-4440-9c8b-d5437f41e7f4",
   "metadata": {},
   "source": [
    "### Load packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fccc7e0-768c-4479-a777-6ce9efbf4d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import datacube\n",
    "import warnings\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from odc.algo import xr_geomedian\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.classification import collect_training_data\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from rasterio.enums import Resampling\n",
    "from random_sampling import random_sampling # adapted from function by Chad Burton: https://gist.github.com/cbur24/04760d645aa123a3b1817b07786e7d9f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efee6d6-c909-427c-bcb7-2943ae33b6bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analysis parameters\n",
    "* `training_features_path`: The path to the file containing training features we extracted through the previous module `0_Extract_Training_Features.ipynb`.\n",
    "* `reference_map_path`: The path to the reference classification map, which will be used as a stratification layer to extract random samples for each class. In this example, we are using the existing national land cover map. **Note that the reference map pixel values should contain the class values existing in the training data.**\n",
    "* `class_attr`: This is the name of column in your shapefile/geojson file attribute table that contains the class labels. **The class labels must be integers**\n",
    "* `output_crs`: Output spatial reference system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62286a65-b4ae-4939-9c35-8b1675899e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features_path = 'Results/Mozambique_training_features.geojson'\n",
    "reference_map_path='Data/moz_lulc2016_28082019_final_remapped_clipped_set_nodata_40m.tif'\n",
    "class_attr = 'LC_Class_I' # class label in integer format\n",
    "output_crs='epsg:32736' # WGS84/UTM Zone 36S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c277e7e2-e195-43b0-9266-77a62813db76",
   "metadata": {},
   "source": [
    "## Load input data\n",
    "\n",
    "We now load the training features file using `geopandas`. The pandas dataframe should contain columns `class_attr` identifying class labels and the bi-monthly geomedians of the nine spectral bands and NDVI that we extracted through previous module. It also contains the coordinates and geometry columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ecf90a7-4fdf-40a9-90fc-edce613c6f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LC_Class_I</th>\n",
       "      <th>blue_0</th>\n",
       "      <th>blue_1</th>\n",
       "      <th>blue_2</th>\n",
       "      <th>blue_3</th>\n",
       "      <th>blue_4</th>\n",
       "      <th>blue_5</th>\n",
       "      <th>green_0</th>\n",
       "      <th>green_1</th>\n",
       "      <th>green_2</th>\n",
       "      <th>...</th>\n",
       "      <th>swir_2_5</th>\n",
       "      <th>NDVI_0</th>\n",
       "      <th>NDVI_1</th>\n",
       "      <th>NDVI_2</th>\n",
       "      <th>NDVI_3</th>\n",
       "      <th>NDVI_4</th>\n",
       "      <th>NDVI_5</th>\n",
       "      <th>x_coord</th>\n",
       "      <th>y_coord</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61.0</td>\n",
       "      <td>1236.0</td>\n",
       "      <td>1139.126953</td>\n",
       "      <td>1048.479736</td>\n",
       "      <td>1063.751343</td>\n",
       "      <td>1165.129028</td>\n",
       "      <td>1292.336792</td>\n",
       "      <td>1942.0</td>\n",
       "      <td>1709.584717</td>\n",
       "      <td>1601.379150</td>\n",
       "      <td>...</td>\n",
       "      <td>4012.578125</td>\n",
       "      <td>0.129363</td>\n",
       "      <td>0.137814</td>\n",
       "      <td>0.144414</td>\n",
       "      <td>0.130792</td>\n",
       "      <td>0.112696</td>\n",
       "      <td>0.090813</td>\n",
       "      <td>692275.0</td>\n",
       "      <td>8605115.0</td>\n",
       "      <td>POINT (692275.000 8605115.000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.0</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>1173.932007</td>\n",
       "      <td>1038.968506</td>\n",
       "      <td>1065.882446</td>\n",
       "      <td>1190.070557</td>\n",
       "      <td>1337.755859</td>\n",
       "      <td>2044.0</td>\n",
       "      <td>1772.987793</td>\n",
       "      <td>1557.223022</td>\n",
       "      <td>...</td>\n",
       "      <td>4012.829346</td>\n",
       "      <td>0.118633</td>\n",
       "      <td>0.120128</td>\n",
       "      <td>0.146507</td>\n",
       "      <td>0.129145</td>\n",
       "      <td>0.104794</td>\n",
       "      <td>0.085257</td>\n",
       "      <td>692275.0</td>\n",
       "      <td>8605105.0</td>\n",
       "      <td>POINT (692275.000 8605105.000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.0</td>\n",
       "      <td>1194.0</td>\n",
       "      <td>1129.869385</td>\n",
       "      <td>1024.202515</td>\n",
       "      <td>1071.311401</td>\n",
       "      <td>1102.611938</td>\n",
       "      <td>1209.021973</td>\n",
       "      <td>1852.0</td>\n",
       "      <td>1587.096436</td>\n",
       "      <td>1554.671509</td>\n",
       "      <td>...</td>\n",
       "      <td>3729.845459</td>\n",
       "      <td>0.155747</td>\n",
       "      <td>0.156303</td>\n",
       "      <td>0.156434</td>\n",
       "      <td>0.130459</td>\n",
       "      <td>0.124384</td>\n",
       "      <td>0.104642</td>\n",
       "      <td>692285.0</td>\n",
       "      <td>8605095.0</td>\n",
       "      <td>POINT (692285.000 8605095.000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>72.0</td>\n",
       "      <td>1556.0</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>280.938965</td>\n",
       "      <td>311.567566</td>\n",
       "      <td>523.496643</td>\n",
       "      <td>538.120789</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>460.000000</td>\n",
       "      <td>530.231445</td>\n",
       "      <td>...</td>\n",
       "      <td>1912.091309</td>\n",
       "      <td>0.379334</td>\n",
       "      <td>0.533181</td>\n",
       "      <td>0.703165</td>\n",
       "      <td>0.623306</td>\n",
       "      <td>0.457564</td>\n",
       "      <td>0.485105</td>\n",
       "      <td>690795.0</td>\n",
       "      <td>8647845.0</td>\n",
       "      <td>POINT (690795.000 8647845.000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72.0</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>473.000000</td>\n",
       "      <td>265.296082</td>\n",
       "      <td>327.585632</td>\n",
       "      <td>533.296875</td>\n",
       "      <td>515.738525</td>\n",
       "      <td>1636.0</td>\n",
       "      <td>426.000000</td>\n",
       "      <td>491.437073</td>\n",
       "      <td>...</td>\n",
       "      <td>1832.404541</td>\n",
       "      <td>0.448956</td>\n",
       "      <td>0.658263</td>\n",
       "      <td>0.708841</td>\n",
       "      <td>0.633997</td>\n",
       "      <td>0.445149</td>\n",
       "      <td>0.493979</td>\n",
       "      <td>690795.0</td>\n",
       "      <td>8647835.0</td>\n",
       "      <td>POINT (690795.000 8647835.000)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LC_Class_I  blue_0       blue_1       blue_2       blue_3       blue_4  \\\n",
       "0        61.0  1236.0  1139.126953  1048.479736  1063.751343  1165.129028   \n",
       "1        61.0  1300.0  1173.932007  1038.968506  1065.882446  1190.070557   \n",
       "2        61.0  1194.0  1129.869385  1024.202515  1071.311401  1102.611938   \n",
       "3        72.0  1556.0   321.000000   280.938965   311.567566   523.496643   \n",
       "4        72.0  1360.0   473.000000   265.296082   327.585632   533.296875   \n",
       "\n",
       "        blue_5  green_0      green_1      green_2  ...     swir_2_5    NDVI_0  \\\n",
       "0  1292.336792   1942.0  1709.584717  1601.379150  ...  4012.578125  0.129363   \n",
       "1  1337.755859   2044.0  1772.987793  1557.223022  ...  4012.829346  0.118633   \n",
       "2  1209.021973   1852.0  1587.096436  1554.671509  ...  3729.845459  0.155747   \n",
       "3   538.120789   1910.0   460.000000   530.231445  ...  1912.091309  0.379334   \n",
       "4   515.738525   1636.0   426.000000   491.437073  ...  1832.404541  0.448956   \n",
       "\n",
       "     NDVI_1    NDVI_2    NDVI_3    NDVI_4    NDVI_5   x_coord    y_coord  \\\n",
       "0  0.137814  0.144414  0.130792  0.112696  0.090813  692275.0  8605115.0   \n",
       "1  0.120128  0.146507  0.129145  0.104794  0.085257  692275.0  8605105.0   \n",
       "2  0.156303  0.156434  0.130459  0.124384  0.104642  692285.0  8605095.0   \n",
       "3  0.533181  0.703165  0.623306  0.457564  0.485105  690795.0  8647845.0   \n",
       "4  0.658263  0.708841  0.633997  0.445149  0.493979  690795.0  8647835.0   \n",
       "\n",
       "                         geometry  \n",
       "0  POINT (692275.000 8605115.000)  \n",
       "1  POINT (692275.000 8605105.000)  \n",
       "2  POINT (692285.000 8605095.000)  \n",
       "3  POINT (690795.000 8647845.000)  \n",
       "4  POINT (690795.000 8647835.000)  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_features= gpd.read_file(training_features_path) # Load training features\n",
    "training_features.head() # Plot first five rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1983c2a7",
   "metadata": {},
   "source": [
    "Using the `class_attr` column we can get the class values, which we will use later to process by class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5266d539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "land cover classes:\n",
      " [61. 72. 41. 75. 44. 74. 31. 12. 71. 21. 70. 51. 11.]\n"
     ]
    }
   ],
   "source": [
    "lc_classes=training_features[class_attr].unique() # get class labels\n",
    "print('land cover classes:\\n',lc_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c42d4cf-ba19-45a8-b095-f42a070e4d92",
   "metadata": {},
   "source": [
    "The training data filtering method also requires a reference land cover map as a stratification layer to generate random training samples, which will be used to train the K-Means models, so We now load the reference map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0486c4df-5048-49ba-a572-90bd8d69d8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference land cover classifcation raster:\n",
      " <xarray.DataArray (y: 45111, x: 28654)>\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)\n",
      "Coordinates:\n",
      "    band         int64 1\n",
      "  * x            (x) float64 2.008e+05 2.008e+05 ... 1.347e+06 1.347e+06\n",
      "  * y            (y) float64 8.832e+06 8.832e+06 ... 7.028e+06 7.028e+06\n",
      "    spatial_ref  int64 ...\n",
      "    variable     <U9 'band_data'\n"
     ]
    }
   ],
   "source": [
    "# load reference classification map\n",
    "reference_map = xr.open_dataset(reference_map_path,engine=\"rasterio\").astype(np.uint8)\n",
    "reference_map=reference_map.to_array().squeeze()\n",
    "print('Reference land cover classifcation raster:\\n',reference_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67922b67",
   "metadata": {},
   "source": [
    "## Generate random samples\n",
    "In many cases there may not be statistically enough samples for some classes in the training data to train the K-Means models. Therefore, we generate some randomly distributed samples for each class from the reference classification map using the `random_sampling` function. This function takes in a few parameters:  \n",
    "* `n`: total number of points to sample\n",
    "* `da`: a classified map as a 2-dimensional xarray.DataArray\n",
    "* `sampling`: the sampling strategy, e.g. 'stratified_random' where each class has a number of points proportional to its relative area, or 'equal_stratified_random' where each class has the same number of points.\n",
    "* `out_fname`: a filepath name for the function to export a shapefile/geojson of the sampling points into a file. You can set this to `None` if you don't need to output the file.\n",
    "* `class_attr`: This is the column name of output dataframe that contains the integer class values on the classified map.\n",
    "* `drop_value`: Pixel value on the classification map to be excluded from sampling.  \n",
    "\n",
    "The output of the function is a geopandas dataframe of randomly distributed points containing a column `class_attr` identifying class values. Here we also re-assgin the other pixel values absent in the training data to the `drop_value` so that these pixels will not be sampled. In this example we excluded 255 (no data values). For a quick demonstration let's sample 1000 pixels in total. To fit in memory we sample over only a subset (5000 by 5000 pixels) of the map. However in your project you need to sample across your study area to make sure the samples are representative of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea1f64ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 11: sampling at 130 coordinates\n",
      "Class 12: sampling at 130 coordinates\n",
      "Class 31: sampling at 130 coordinates\n",
      "Class 41: sampling at 130 coordinates\n",
      "Class 44: sampling at 130 coordinates\n",
      "Class 51: sampling at 130 coordinates\n",
      "Class 61: sampling at 130 coordinates\n",
      "Class 72: sampling at 130 coordinates\n",
      "Class 74: sampling at 130 coordinates\n",
      "Class 75: sampling at 130 coordinates\n"
     ]
    }
   ],
   "source": [
    "# da=reference_map.where((reference_map!=0)&(reference_map!=3)&(reference_map!=255),np.nan)\n",
    "da=reference_map.where(reference_map!=255,0)\n",
    "gpd_random_samples=random_sampling(da[10000:15000,10000:15000],n=1300,sampling='equal_stratified_random',\n",
    "                                   out_fname=None,class_attr=class_attr,drop_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8347a6c-f41a-428c-8eb0-05bbcb9a0746",
   "metadata": {},
   "source": [
    "In this example we have generated ~1000 samples for each class across Rwanda, i.e. a total of 8000 random samples were generated. The points are stored in the file 'Results/Rwanda_random_samples.geojson'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a07c8c5",
   "metadata": {},
   "source": [
    "## Extract features\n",
    "With the random sample points available, we now need to extract features to train the K-Means models. As we will apply clustering on all the training features that were extracted through the previous module `0_Extract_Training_Features.ipynb`, we can re-use the query and feature layer function in the previous notebook to extract the features, i.e. bi-monthly geomedian of the nine spectral bands and NDVI. As we have demonstrated how to extract training features in the previous module, in this example we skip it but use a prepared file of extracted features for the random samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bdfce2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LC_Class_I</th>\n",
       "      <th>blue_0</th>\n",
       "      <th>blue_1</th>\n",
       "      <th>blue_2</th>\n",
       "      <th>blue_3</th>\n",
       "      <th>blue_4</th>\n",
       "      <th>blue_5</th>\n",
       "      <th>green_0</th>\n",
       "      <th>green_1</th>\n",
       "      <th>green_2</th>\n",
       "      <th>...</th>\n",
       "      <th>swir_2_3</th>\n",
       "      <th>swir_2_4</th>\n",
       "      <th>swir_2_5</th>\n",
       "      <th>NDVI_0</th>\n",
       "      <th>NDVI_1</th>\n",
       "      <th>NDVI_2</th>\n",
       "      <th>NDVI_3</th>\n",
       "      <th>NDVI_4</th>\n",
       "      <th>NDVI_5</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51.0</td>\n",
       "      <td>411.907104</td>\n",
       "      <td>516.000000</td>\n",
       "      <td>697.457886</td>\n",
       "      <td>707.468506</td>\n",
       "      <td>876.202271</td>\n",
       "      <td>920.230652</td>\n",
       "      <td>793.171814</td>\n",
       "      <td>740.000000</td>\n",
       "      <td>1007.683899</td>\n",
       "      <td>...</td>\n",
       "      <td>2359.454834</td>\n",
       "      <td>2515.819336</td>\n",
       "      <td>2442.764648</td>\n",
       "      <td>0.657608</td>\n",
       "      <td>0.431102</td>\n",
       "      <td>0.208764</td>\n",
       "      <td>0.157779</td>\n",
       "      <td>0.130523</td>\n",
       "      <td>0.091194</td>\n",
       "      <td>POINT (650905.000 8390145.000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51.0</td>\n",
       "      <td>577.000000</td>\n",
       "      <td>685.722961</td>\n",
       "      <td>534.333618</td>\n",
       "      <td>632.182129</td>\n",
       "      <td>802.583313</td>\n",
       "      <td>770.282715</td>\n",
       "      <td>921.000000</td>\n",
       "      <td>1065.882202</td>\n",
       "      <td>829.994812</td>\n",
       "      <td>...</td>\n",
       "      <td>2593.210693</td>\n",
       "      <td>2902.564697</td>\n",
       "      <td>2952.701904</td>\n",
       "      <td>0.358475</td>\n",
       "      <td>0.479083</td>\n",
       "      <td>0.316647</td>\n",
       "      <td>0.196990</td>\n",
       "      <td>0.164418</td>\n",
       "      <td>0.151651</td>\n",
       "      <td>POINT (652195.000 8358135.000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51.0</td>\n",
       "      <td>592.908264</td>\n",
       "      <td>608.015015</td>\n",
       "      <td>736.447327</td>\n",
       "      <td>787.725098</td>\n",
       "      <td>939.028564</td>\n",
       "      <td>796.394592</td>\n",
       "      <td>1056.399170</td>\n",
       "      <td>1004.525391</td>\n",
       "      <td>1198.688965</td>\n",
       "      <td>...</td>\n",
       "      <td>2577.558105</td>\n",
       "      <td>2981.744141</td>\n",
       "      <td>2647.535645</td>\n",
       "      <td>0.657485</td>\n",
       "      <td>0.487768</td>\n",
       "      <td>0.321184</td>\n",
       "      <td>0.267627</td>\n",
       "      <td>0.235933</td>\n",
       "      <td>0.322459</td>\n",
       "      <td>POINT (641905.000 8372025.000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51.0</td>\n",
       "      <td>528.001343</td>\n",
       "      <td>865.206909</td>\n",
       "      <td>551.653687</td>\n",
       "      <td>720.208862</td>\n",
       "      <td>920.541626</td>\n",
       "      <td>1021.595337</td>\n",
       "      <td>874.000916</td>\n",
       "      <td>986.782837</td>\n",
       "      <td>819.995483</td>\n",
       "      <td>...</td>\n",
       "      <td>2417.149902</td>\n",
       "      <td>2792.781494</td>\n",
       "      <td>2931.050781</td>\n",
       "      <td>0.586413</td>\n",
       "      <td>0.509176</td>\n",
       "      <td>0.368637</td>\n",
       "      <td>0.223229</td>\n",
       "      <td>0.185781</td>\n",
       "      <td>0.185765</td>\n",
       "      <td>POINT (661975.000 8389575.000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.0</td>\n",
       "      <td>492.875610</td>\n",
       "      <td>570.037659</td>\n",
       "      <td>412.392670</td>\n",
       "      <td>574.462219</td>\n",
       "      <td>731.276001</td>\n",
       "      <td>580.618896</td>\n",
       "      <td>820.530212</td>\n",
       "      <td>769.601318</td>\n",
       "      <td>718.130981</td>\n",
       "      <td>...</td>\n",
       "      <td>2213.340332</td>\n",
       "      <td>2793.541504</td>\n",
       "      <td>2990.641357</td>\n",
       "      <td>0.803368</td>\n",
       "      <td>0.599269</td>\n",
       "      <td>0.521683</td>\n",
       "      <td>0.286149</td>\n",
       "      <td>0.224348</td>\n",
       "      <td>0.259709</td>\n",
       "      <td>POINT (641005.000 8380965.000)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LC_Class_I      blue_0      blue_1      blue_2      blue_3      blue_4  \\\n",
       "0        51.0  411.907104  516.000000  697.457886  707.468506  876.202271   \n",
       "1        51.0  577.000000  685.722961  534.333618  632.182129  802.583313   \n",
       "2        51.0  592.908264  608.015015  736.447327  787.725098  939.028564   \n",
       "3        51.0  528.001343  865.206909  551.653687  720.208862  920.541626   \n",
       "4        51.0  492.875610  570.037659  412.392670  574.462219  731.276001   \n",
       "\n",
       "        blue_5      green_0      green_1      green_2  ...     swir_2_3  \\\n",
       "0   920.230652   793.171814   740.000000  1007.683899  ...  2359.454834   \n",
       "1   770.282715   921.000000  1065.882202   829.994812  ...  2593.210693   \n",
       "2   796.394592  1056.399170  1004.525391  1198.688965  ...  2577.558105   \n",
       "3  1021.595337   874.000916   986.782837   819.995483  ...  2417.149902   \n",
       "4   580.618896   820.530212   769.601318   718.130981  ...  2213.340332   \n",
       "\n",
       "      swir_2_4     swir_2_5    NDVI_0    NDVI_1    NDVI_2    NDVI_3    NDVI_4  \\\n",
       "0  2515.819336  2442.764648  0.657608  0.431102  0.208764  0.157779  0.130523   \n",
       "1  2902.564697  2952.701904  0.358475  0.479083  0.316647  0.196990  0.164418   \n",
       "2  2981.744141  2647.535645  0.657485  0.487768  0.321184  0.267627  0.235933   \n",
       "3  2792.781494  2931.050781  0.586413  0.509176  0.368637  0.223229  0.185781   \n",
       "4  2793.541504  2990.641357  0.803368  0.599269  0.521683  0.286149  0.224348   \n",
       "\n",
       "     NDVI_5                        geometry  \n",
       "0  0.091194  POINT (650905.000 8390145.000)  \n",
       "1  0.151651  POINT (652195.000 8358135.000)  \n",
       "2  0.322459  POINT (641905.000 8372025.000)  \n",
       "3  0.185765  POINT (661975.000 8389575.000)  \n",
       "4  0.259709  POINT (641005.000 8380965.000)  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_samples_features_path='Results/stratified_random_samples_signatures_using_lulc2016.geojson'\n",
    "rand_samples_features=gpd.read_file(rand_samples_features_path)\n",
    "rand_samples_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50021c25-ec7c-477f-8833-31bf6a44927a",
   "metadata": {},
   "source": [
    "## K-Means clustering\n",
    "Now that we have the features of random samples and training points, we can use them to train and apply the K-Means models. The K-Means model requires a pre-defined number of clusters, which is unknown for many cases. One way to identify the optimal number of clusters is using the Calinski-Harabasz Index. The index is the ratio of the sum of between-clusters dispersion and of within-cluster dispersion for all clusters, where the index is higher when clusters are dense and well separated. More information about can be checked [here](https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index). In this example we calculate the indices calculated from clustering with a varied number of clusters (e.g. 3 to 20) and retain the clustering with the highest index.  \n",
    "> Note: You can also use other indices to assess the clustering and choose optimal number of clusterings, see information on other indices [here](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation). Depending on the distribution of you features, different indices may lead to different optimal cluster numbers. \n",
    "\n",
    "Here we put the procedures in identifying the optimal clustering into a function where the inputs are the input features, minimum and maximum number of clusters, and the outputs are the optimal number of clusters, trained K-Means model and corresponding clustering labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18f05f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clusters_KMeans(data,min_cluster=3,max_cluster=20):\n",
    "    highest_score=-999\n",
    "    n_cluster_optimal=min_cluster\n",
    "    kmeans_model_optimal=None # initialise optimal model parameters\n",
    "    labels_optimal=None\n",
    "    for n_cluster in range(min_cluster,max_cluster):\n",
    "        kmeans_model = KMeans(n_clusters=n_cluster, random_state=1).fit(data)\n",
    "        labels=kmeans_model.predict(data)\n",
    "        score=metrics.calinski_harabasz_score(data, labels)\n",
    "        print('Calinski-Harabasz score for ',n_cluster,' clusters is: ',score)\n",
    "        if (highest_score==-999)or(highest_score<score):\n",
    "            highest_score=score\n",
    "            n_cluster_optimal=n_cluster\n",
    "            kmeans_model_optimal=kmeans_model\n",
    "            labels_optimal=labels\n",
    "    print('Best number of clusters: %s'%(n_cluster_optimal))\n",
    "    return n_cluster_optimal,kmeans_model_optimal,labels_optimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0835f9cf",
   "metadata": {},
   "source": [
    "Using the above function, we now cluster the training features for the first class as an example. We first retain the random sample and training sample features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e8102770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training pints for the class:  5170\n"
     ]
    }
   ],
   "source": [
    "# get class label\n",
    "i=lc_classes[0]\n",
    "# subset random sample features for this class\n",
    "rand_features_single_class=rand_samples_features[rand_samples_features[class_attr]==i].reset_index(drop=True)\n",
    "# subset original training points for this class\n",
    "td_single_class=training_features[training_features[class_attr]==i].reset_index(drop=True)\n",
    "print('Number of training pints for the class: ',len(td_single_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8991942",
   "metadata": {},
   "source": [
    "We then apply the `find_clusters_KMeans` function to the random sample features to find optimal clustering. Note that K-Means model is sensitive to feature scales, so we need to standardise all features before applying the model. Here we use scikitlearn `StandardScaler` to implement the feature standardisation. Remember to drop coordinates and geometry columns from the features for the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "993a77ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calinski-Harabasz score for  3  clusters is:  394.3823057283652\n",
      "Calinski-Harabasz score for  4  clusters is:  371.9323704020279\n",
      "Calinski-Harabasz score for  5  clusters is:  339.7397807862018\n",
      "Calinski-Harabasz score for  6  clusters is:  302.84279441348315\n",
      "Calinski-Harabasz score for  7  clusters is:  277.03937453672455\n",
      "Calinski-Harabasz score for  8  clusters is:  258.30860348479297\n",
      "Calinski-Harabasz score for  9  clusters is:  238.42932537098477\n",
      "Calinski-Harabasz score for  10  clusters is:  226.67365273953874\n",
      "Calinski-Harabasz score for  11  clusters is:  214.43594280139683\n",
      "Calinski-Harabasz score for  12  clusters is:  205.14060918943235\n",
      "Calinski-Harabasz score for  13  clusters is:  194.3908295739775\n",
      "Calinski-Harabasz score for  14  clusters is:  185.7143128599663\n",
      "Calinski-Harabasz score for  15  clusters is:  180.60024245728974\n",
      "Calinski-Harabasz score for  16  clusters is:  171.8730040917805\n",
      "Calinski-Harabasz score for  17  clusters is:  167.83801606201345\n",
      "Calinski-Harabasz score for  18  clusters is:  161.964852088467\n",
      "Calinski-Harabasz score for  19  clusters is:  156.52767612530343\n",
      "Best number of clusters: 3\n"
     ]
    }
   ],
   "source": [
    "# initialise standard scaler\n",
    "scaler = StandardScaler()\n",
    "# fit random samples\n",
    "scaler.fit(rand_features_single_class.iloc[:,1:-1])\n",
    "# transform random samples\n",
    "rand_features_single_class=scaler.transform(rand_features_single_class.iloc[:,1:-1])\n",
    "# find optimal clustering\n",
    "n_cluster_optimal,kmeans_model_optimal,labels_optimal=find_clusters_KMeans(rand_features_single_class,min_cluster=3,max_cluster=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e133aa86",
   "metadata": {},
   "source": [
    "After identifying the optimal clustering, we can apply the optimal K-Means model to our training features. Remember to apply feature standardisation before implementing the clustering. Here we assign the clustering labels to a new column `cluster`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe8e1ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalisation before clustering\n",
    "model_input=scaler.transform(td_single_class.iloc[:,1:-3])\n",
    "# predict clustering labels\n",
    "labels_kmeans = kmeans_model_optimal.predict(model_input)\n",
    "# append clustering results to pixel coordinates\n",
    "td_single_class['cluster']=labels_kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d34d72",
   "metadata": {},
   "source": [
    "## Filtering training features\n",
    "\n",
    "We now filter the training features/points based on the cluster size. Here we assume cluster size lower than 5% of the overall sample szie are likely to be misclassified or changed samples.    \n",
    ">Note: Depending on your own training data the K-Means method may not work well, so it is recommanded that you have more understanding on your training points and test on how it works, e.g. check if it successfully filtered out the points you believe were misclassified while keeping good training samples. You should also try to adjust the cluster size threshold if it doesn't effectively remove false samples.\n",
    "\n",
    "There are also other options for removal of outliers which can be tested on, e.g. check [here](https://scikit-learn.org/stable/modules/outlier_detection.html) for using scikitlearn for outlier detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd2f7c96-6bbb-4670-a300-24bf389d72bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data after filtering:  5170\n"
     ]
    }
   ],
   "source": [
    "frequency_threshold=0.05 # threshold of cluter frequency\n",
    "cluster_frequency=td_single_class['cluster'].map(td_single_class['cluster'].value_counts(normalize=True)) # calculate cluster frequencies for the training samples\n",
    "td_single_class['cluster_frequency']=cluster_frequency # append as a column\n",
    "td_single_class_filtered=td_single_class[td_single_class['cluster_frequency']>=frequency_threshold] # filter by cluster frequency\n",
    "print('Number of training data after filtering: ',len(td_single_class_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da70a73",
   "metadata": {},
   "source": [
    "You can compare the number of training points before and after the filtering and check whether and how many pixels were filtered out. To implement above clustering and filtering training features for all class, let's put the steps together and iterate through all classes. Here we append filtered features for all classes into a single dataframe `training_features_filtered`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4fd905a2-2137-4fb0-8c4b-3098c8c96b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class  61.0\n",
      "Number of training pints for the class:  5170\n",
      "Calinski-Harabasz score for  3  clusters is:  394.3823057283652\n",
      "Calinski-Harabasz score for  4  clusters is:  371.9323704020279\n",
      "Calinski-Harabasz score for  5  clusters is:  339.7397807862018\n",
      "Calinski-Harabasz score for  6  clusters is:  302.84279441348315\n",
      "Calinski-Harabasz score for  7  clusters is:  277.03937453672455\n",
      "Calinski-Harabasz score for  8  clusters is:  258.30860348479297\n",
      "Calinski-Harabasz score for  9  clusters is:  238.42932537098477\n",
      "Calinski-Harabasz score for  10  clusters is:  226.67365273953874\n",
      "Calinski-Harabasz score for  11  clusters is:  214.43594280139683\n",
      "Calinski-Harabasz score for  12  clusters is:  205.14060918943235\n",
      "Calinski-Harabasz score for  13  clusters is:  194.3908295739775\n",
      "Calinski-Harabasz score for  14  clusters is:  185.7143128599663\n",
      "Calinski-Harabasz score for  15  clusters is:  180.60024245728974\n",
      "Calinski-Harabasz score for  16  clusters is:  171.8730040917805\n",
      "Calinski-Harabasz score for  17  clusters is:  167.83801606201345\n",
      "Calinski-Harabasz score for  18  clusters is:  161.964852088467\n",
      "Calinski-Harabasz score for  19  clusters is:  156.52767612530343\n",
      "Best number of clusters: 3\n",
      "Number of training data after filtering:  5170\n",
      "Processing class  72.0\n",
      "Number of training pints for the class:  8378\n",
      "Calinski-Harabasz score for  3  clusters is:  253.7762602096454\n",
      "Calinski-Harabasz score for  4  clusters is:  232.234415581989\n",
      "Calinski-Harabasz score for  5  clusters is:  213.41531631856256\n",
      "Calinski-Harabasz score for  6  clusters is:  202.31693279033945\n",
      "Calinski-Harabasz score for  7  clusters is:  191.02702393679667\n",
      "Calinski-Harabasz score for  8  clusters is:  180.526320197678\n",
      "Calinski-Harabasz score for  9  clusters is:  168.91778554756883\n",
      "Calinski-Harabasz score for  10  clusters is:  158.82969858125963\n",
      "Calinski-Harabasz score for  11  clusters is:  148.61645190319146\n",
      "Calinski-Harabasz score for  12  clusters is:  141.97643109967686\n",
      "Calinski-Harabasz score for  13  clusters is:  135.1501559129529\n",
      "Calinski-Harabasz score for  14  clusters is:  129.48124146862588\n",
      "Calinski-Harabasz score for  15  clusters is:  124.4290928359594\n",
      "Calinski-Harabasz score for  16  clusters is:  119.75300850374157\n",
      "Calinski-Harabasz score for  17  clusters is:  114.8109979878626\n",
      "Calinski-Harabasz score for  18  clusters is:  110.46986720825787\n",
      "Calinski-Harabasz score for  19  clusters is:  107.71361822147426\n",
      "Best number of clusters: 3\n",
      "Number of training data after filtering:  8301\n",
      "Processing class  41.0\n",
      "Number of training pints for the class:  4270\n",
      "Calinski-Harabasz score for  3  clusters is:  182.23614400425177\n",
      "Calinski-Harabasz score for  4  clusters is:  167.0532280986277\n",
      "Calinski-Harabasz score for  5  clusters is:  150.36628583090524\n",
      "Calinski-Harabasz score for  6  clusters is:  135.86327859273896\n",
      "Calinski-Harabasz score for  7  clusters is:  127.19493192561033\n",
      "Calinski-Harabasz score for  8  clusters is:  120.06956799651411\n",
      "Calinski-Harabasz score for  9  clusters is:  114.86925730966982\n",
      "Calinski-Harabasz score for  10  clusters is:  109.01035096247955\n",
      "Calinski-Harabasz score for  11  clusters is:  103.41769042268712\n",
      "Calinski-Harabasz score for  12  clusters is:  99.97121360448853\n",
      "Calinski-Harabasz score for  13  clusters is:  97.42953981820632\n",
      "Calinski-Harabasz score for  14  clusters is:  93.26864511283077\n",
      "Calinski-Harabasz score for  15  clusters is:  90.67856795839924\n",
      "Calinski-Harabasz score for  16  clusters is:  87.52806026733042\n",
      "Calinski-Harabasz score for  17  clusters is:  83.84623890894873\n",
      "Calinski-Harabasz score for  18  clusters is:  82.3902484695037\n",
      "Calinski-Harabasz score for  19  clusters is:  79.62314118423669\n",
      "Best number of clusters: 3\n",
      "Number of training data after filtering:  4270\n",
      "Processing class  75.0\n",
      "Number of training pints for the class:  2433\n",
      "Calinski-Harabasz score for  3  clusters is:  252.02630031170546\n",
      "Calinski-Harabasz score for  4  clusters is:  209.82195613894\n",
      "Calinski-Harabasz score for  5  clusters is:  180.90962413897358\n",
      "Calinski-Harabasz score for  6  clusters is:  162.7642592952096\n",
      "Calinski-Harabasz score for  7  clusters is:  148.94840927443985\n",
      "Calinski-Harabasz score for  8  clusters is:  139.02364813249454\n",
      "Calinski-Harabasz score for  9  clusters is:  133.40765203710552\n",
      "Calinski-Harabasz score for  10  clusters is:  126.55118537757481\n",
      "Calinski-Harabasz score for  11  clusters is:  119.54489658444025\n",
      "Calinski-Harabasz score for  12  clusters is:  113.29579364382538\n",
      "Calinski-Harabasz score for  13  clusters is:  107.24082257641496\n",
      "Calinski-Harabasz score for  14  clusters is:  102.66030933316483\n",
      "Calinski-Harabasz score for  15  clusters is:  98.34760205898998\n",
      "Calinski-Harabasz score for  16  clusters is:  94.88580887014461\n",
      "Calinski-Harabasz score for  17  clusters is:  91.31572486282747\n",
      "Calinski-Harabasz score for  18  clusters is:  88.76509324241935\n",
      "Calinski-Harabasz score for  19  clusters is:  85.21360229550491\n",
      "Best number of clusters: 3\n",
      "Number of training data after filtering:  2433\n",
      "Processing class  44.0\n",
      "Number of training pints for the class:  1879\n",
      "Calinski-Harabasz score for  3  clusters is:  716.4724587026383\n",
      "Calinski-Harabasz score for  4  clusters is:  753.6174853991722\n",
      "Calinski-Harabasz score for  5  clusters is:  733.430955096481\n",
      "Calinski-Harabasz score for  6  clusters is:  674.5090213405931\n",
      "Calinski-Harabasz score for  7  clusters is:  625.3337356361181\n",
      "Calinski-Harabasz score for  8  clusters is:  584.0285765882093\n",
      "Calinski-Harabasz score for  9  clusters is:  562.1457138247317\n",
      "Calinski-Harabasz score for  10  clusters is:  529.7820595270629\n",
      "Calinski-Harabasz score for  11  clusters is:  504.1108749058042\n",
      "Calinski-Harabasz score for  12  clusters is:  484.6862985700011\n",
      "Calinski-Harabasz score for  13  clusters is:  469.28897391867906\n",
      "Calinski-Harabasz score for  14  clusters is:  454.94142714982684\n",
      "Calinski-Harabasz score for  15  clusters is:  442.06105352905956\n",
      "Calinski-Harabasz score for  16  clusters is:  426.4785865292115\n",
      "Calinski-Harabasz score for  17  clusters is:  418.24199665484736\n",
      "Calinski-Harabasz score for  18  clusters is:  413.7245413984544\n",
      "Calinski-Harabasz score for  19  clusters is:  397.8365814456338\n",
      "Best number of clusters: 4\n",
      "Number of training data after filtering:  1879\n",
      "Processing class  74.0\n",
      "Number of training pints for the class:  4923\n",
      "Calinski-Harabasz score for  3  clusters is:  211.6953087905795\n",
      "Calinski-Harabasz score for  4  clusters is:  173.14615188951424\n",
      "Calinski-Harabasz score for  5  clusters is:  150.83588142439513\n",
      "Calinski-Harabasz score for  6  clusters is:  134.7863340341997\n",
      "Calinski-Harabasz score for  7  clusters is:  123.38506915824183\n",
      "Calinski-Harabasz score for  8  clusters is:  115.78856728480434\n",
      "Calinski-Harabasz score for  9  clusters is:  108.7692342412636\n",
      "Calinski-Harabasz score for  10  clusters is:  102.86193549311953\n",
      "Calinski-Harabasz score for  11  clusters is:  96.98931314469493\n",
      "Calinski-Harabasz score for  12  clusters is:  92.06373849480909\n",
      "Calinski-Harabasz score for  13  clusters is:  87.62109201926657\n",
      "Calinski-Harabasz score for  14  clusters is:  84.09897960769173\n",
      "Calinski-Harabasz score for  15  clusters is:  80.61552728130977\n",
      "Calinski-Harabasz score for  16  clusters is:  77.05378873647945\n",
      "Calinski-Harabasz score for  17  clusters is:  74.06403378052704\n",
      "Calinski-Harabasz score for  18  clusters is:  71.2993173225331\n",
      "Calinski-Harabasz score for  19  clusters is:  68.92496889075059\n",
      "Best number of clusters: 3\n",
      "Number of training data after filtering:  4923\n",
      "Processing class  31.0\n",
      "Number of training pints for the class:  5734\n",
      "Calinski-Harabasz score for  3  clusters is:  210.78407109737333\n",
      "Calinski-Harabasz score for  4  clusters is:  177.5760517247893\n",
      "Calinski-Harabasz score for  5  clusters is:  153.36649207067927\n",
      "Calinski-Harabasz score for  6  clusters is:  137.8036332332944\n",
      "Calinski-Harabasz score for  7  clusters is:  125.34631917901322\n",
      "Calinski-Harabasz score for  8  clusters is:  114.71126406257214\n",
      "Calinski-Harabasz score for  9  clusters is:  105.76480042951765\n",
      "Calinski-Harabasz score for  10  clusters is:  98.56816296629516\n",
      "Calinski-Harabasz score for  11  clusters is:  93.8990575076536\n",
      "Calinski-Harabasz score for  12  clusters is:  88.41120908736437\n",
      "Calinski-Harabasz score for  13  clusters is:  85.48748772208772\n",
      "Calinski-Harabasz score for  14  clusters is:  81.42593688876184\n",
      "Calinski-Harabasz score for  15  clusters is:  78.7793998524576\n",
      "Calinski-Harabasz score for  16  clusters is:  74.87358402040066\n",
      "Calinski-Harabasz score for  17  clusters is:  73.20184959599618\n",
      "Calinski-Harabasz score for  18  clusters is:  70.66120550243441\n",
      "Calinski-Harabasz score for  19  clusters is:  67.88091424264273\n",
      "Best number of clusters: 3\n",
      "Number of training data after filtering:  5734\n",
      "Processing class  12.0\n",
      "Number of training pints for the class:  4436\n",
      "Calinski-Harabasz score for  3  clusters is:  284.03045240548977\n",
      "Calinski-Harabasz score for  4  clusters is:  240.11492303864208\n",
      "Calinski-Harabasz score for  5  clusters is:  205.96397712845015\n",
      "Calinski-Harabasz score for  6  clusters is:  182.20114293813302\n",
      "Calinski-Harabasz score for  7  clusters is:  162.68676399314535\n",
      "Calinski-Harabasz score for  8  clusters is:  148.22590946462918\n",
      "Calinski-Harabasz score for  9  clusters is:  135.8509167761165\n",
      "Calinski-Harabasz score for  10  clusters is:  127.94755694516185\n",
      "Calinski-Harabasz score for  11  clusters is:  119.4882461491071\n",
      "Calinski-Harabasz score for  12  clusters is:  113.91899731274653\n",
      "Calinski-Harabasz score for  13  clusters is:  108.03115487949337\n",
      "Calinski-Harabasz score for  14  clusters is:  103.02203288620177\n",
      "Calinski-Harabasz score for  15  clusters is:  99.25923075898132\n",
      "Calinski-Harabasz score for  16  clusters is:  94.50100097774876\n",
      "Calinski-Harabasz score for  17  clusters is:  90.74944238830778\n",
      "Calinski-Harabasz score for  18  clusters is:  88.15220223940389\n",
      "Calinski-Harabasz score for  19  clusters is:  85.17302266426088\n",
      "Best number of clusters: 3\n",
      "Number of training data after filtering:  4436\n",
      "Processing class  71.0\n",
      "Number of training pints for the class:  1155\n",
      "Calinski-Harabasz score for  3  clusters is:  390.8475417889016\n",
      "Calinski-Harabasz score for  4  clusters is:  334.7225845980191\n",
      "Calinski-Harabasz score for  5  clusters is:  293.13777076076343\n",
      "Calinski-Harabasz score for  6  clusters is:  263.4381119581924\n",
      "Calinski-Harabasz score for  7  clusters is:  241.40542856396604\n",
      "Calinski-Harabasz score for  8  clusters is:  226.38614768459877\n",
      "Calinski-Harabasz score for  9  clusters is:  211.0988265161667\n",
      "Calinski-Harabasz score for  10  clusters is:  199.0270925190744\n",
      "Calinski-Harabasz score for  11  clusters is:  191.99065534491186\n",
      "Calinski-Harabasz score for  12  clusters is:  181.91754983500184\n",
      "Calinski-Harabasz score for  13  clusters is:  173.6801445460491\n",
      "Calinski-Harabasz score for  14  clusters is:  165.8985182602577\n",
      "Calinski-Harabasz score for  15  clusters is:  158.1131430499253\n",
      "Calinski-Harabasz score for  16  clusters is:  150.3332951616581\n",
      "Calinski-Harabasz score for  17  clusters is:  146.60765225914693\n",
      "Calinski-Harabasz score for  18  clusters is:  139.65964369907286\n",
      "Calinski-Harabasz score for  19  clusters is:  134.15535378154124\n",
      "Best number of clusters: 3\n",
      "Number of training data after filtering:  1144\n",
      "Processing class  21.0\n",
      "Number of training pints for the class:  1641\n",
      "Calinski-Harabasz score for  3  clusters is:  313.5154718315559\n",
      "Calinski-Harabasz score for  4  clusters is:  259.13882992777195\n",
      "Calinski-Harabasz score for  5  clusters is:  222.38294693911993\n",
      "Calinski-Harabasz score for  6  clusters is:  198.38855771032965\n",
      "Calinski-Harabasz score for  7  clusters is:  183.3045686917165\n",
      "Calinski-Harabasz score for  8  clusters is:  173.53619229833959\n",
      "Calinski-Harabasz score for  9  clusters is:  161.75626076884492\n",
      "Calinski-Harabasz score for  10  clusters is:  150.99980562445592\n",
      "Calinski-Harabasz score for  11  clusters is:  142.7529021702552\n",
      "Calinski-Harabasz score for  12  clusters is:  134.44276911463723\n",
      "Calinski-Harabasz score for  13  clusters is:  127.7026577780812\n",
      "Calinski-Harabasz score for  14  clusters is:  122.08261755474607\n",
      "Calinski-Harabasz score for  15  clusters is:  116.9524969948943\n",
      "Calinski-Harabasz score for  16  clusters is:  112.37805936841048\n",
      "Calinski-Harabasz score for  17  clusters is:  109.58936743339962\n",
      "Calinski-Harabasz score for  18  clusters is:  104.43362175638005\n",
      "Calinski-Harabasz score for  19  clusters is:  101.60402984690482\n",
      "Best number of clusters: 3\n",
      "Number of training data after filtering:  1641\n",
      "Processing class  70.0\n",
      "Number of training pints for the class:  1966\n",
      "Calinski-Harabasz score for  3  clusters is:  388.3815146768992\n",
      "Calinski-Harabasz score for  4  clusters is:  344.10053315131125\n",
      "Calinski-Harabasz score for  5  clusters is:  316.6341842036556\n",
      "Calinski-Harabasz score for  6  clusters is:  287.0669317850765\n",
      "Calinski-Harabasz score for  7  clusters is:  268.2862064156189\n",
      "Calinski-Harabasz score for  8  clusters is:  257.0836566854\n",
      "Calinski-Harabasz score for  9  clusters is:  242.562095226382\n",
      "Calinski-Harabasz score for  10  clusters is:  230.610990795816\n",
      "Calinski-Harabasz score for  11  clusters is:  221.69134134902413\n",
      "Calinski-Harabasz score for  12  clusters is:  208.987607831039\n",
      "Calinski-Harabasz score for  13  clusters is:  201.7172174224359\n",
      "Calinski-Harabasz score for  14  clusters is:  191.60039909444757\n",
      "Calinski-Harabasz score for  15  clusters is:  186.48131396123375\n",
      "Calinski-Harabasz score for  16  clusters is:  177.1884224144564\n",
      "Calinski-Harabasz score for  17  clusters is:  171.70479559381303\n",
      "Calinski-Harabasz score for  18  clusters is:  168.1379497379875\n",
      "Calinski-Harabasz score for  19  clusters is:  164.4426111132564\n",
      "Best number of clusters: 3\n",
      "Number of training data after filtering:  1966\n",
      "Processing class  51.0\n",
      "Number of training pints for the class:  1400\n",
      "Calinski-Harabasz score for  3  clusters is:  355.5610494606655\n",
      "Calinski-Harabasz score for  4  clusters is:  298.84848013021804\n",
      "Calinski-Harabasz score for  5  clusters is:  260.4169474378255\n",
      "Calinski-Harabasz score for  6  clusters is:  235.58106918364092\n",
      "Calinski-Harabasz score for  7  clusters is:  212.79681313932159\n",
      "Calinski-Harabasz score for  8  clusters is:  198.13012568934676\n",
      "Calinski-Harabasz score for  9  clusters is:  182.9631501723792\n",
      "Calinski-Harabasz score for  10  clusters is:  171.17056104258265\n",
      "Calinski-Harabasz score for  11  clusters is:  160.4942717563735\n",
      "Calinski-Harabasz score for  12  clusters is:  150.70158157112093\n",
      "Calinski-Harabasz score for  13  clusters is:  141.77251274173108\n",
      "Calinski-Harabasz score for  14  clusters is:  134.35900167582994\n",
      "Calinski-Harabasz score for  15  clusters is:  127.29821139629576\n",
      "Calinski-Harabasz score for  16  clusters is:  121.41759072617869\n",
      "Calinski-Harabasz score for  17  clusters is:  116.74795948220806\n",
      "Calinski-Harabasz score for  18  clusters is:  112.76797367314853\n",
      "Calinski-Harabasz score for  19  clusters is:  107.86316567184471\n",
      "Best number of clusters: 3\n",
      "Number of training data after filtering:  1400\n",
      "Processing class  11.0\n",
      "Number of training pints for the class:  854\n",
      "Calinski-Harabasz score for  3  clusters is:  334.99157591932817\n",
      "Calinski-Harabasz score for  4  clusters is:  283.9913688466575\n",
      "Calinski-Harabasz score for  5  clusters is:  244.38252368453917\n",
      "Calinski-Harabasz score for  6  clusters is:  216.92858359215916\n",
      "Calinski-Harabasz score for  7  clusters is:  195.51433080863487\n",
      "Calinski-Harabasz score for  8  clusters is:  179.7875375190479\n",
      "Calinski-Harabasz score for  9  clusters is:  168.92338256033835\n",
      "Calinski-Harabasz score for  10  clusters is:  158.6299685261019\n",
      "Calinski-Harabasz score for  11  clusters is:  149.75447873627482\n",
      "Calinski-Harabasz score for  12  clusters is:  141.31906332455597\n",
      "Calinski-Harabasz score for  13  clusters is:  133.91420496830256\n",
      "Calinski-Harabasz score for  14  clusters is:  128.54625731319214\n",
      "Calinski-Harabasz score for  15  clusters is:  121.74426237161141\n",
      "Calinski-Harabasz score for  16  clusters is:  115.71014141647387\n",
      "Calinski-Harabasz score for  17  clusters is:  112.3842062109019\n",
      "Calinski-Harabasz score for  18  clusters is:  108.05376718575467\n",
      "Calinski-Harabasz score for  19  clusters is:  103.58715236994084\n",
      "Best number of clusters: 3\n",
      "Number of training data after filtering:  854\n"
     ]
    }
   ],
   "source": [
    "training_features_filtered=None # filtered training data for all classes\n",
    "scaler = StandardScaler() # initialise standard scaler\n",
    "frequency_threshold=0.05 # threshold of cluter frequency\n",
    "for i in lc_classes: # filtering training data for each class\n",
    "    #i=1 # test for first class\n",
    "    print('Processing class ',i)\n",
    "    # subset random sample features for this class\n",
    "    rand_features_single_class=rand_samples_features[rand_samples_features[class_attr]==i].reset_index(drop=True)\n",
    "    # subset original training points for this class\n",
    "    td_single_class=training_features[training_features[class_attr]==i].reset_index(drop=True)\n",
    "    print('Number of training pints for the class: ',len(td_single_class))\n",
    "    # fit random samples\n",
    "    scaler.fit(rand_features_single_class.iloc[:,1:-1])\n",
    "    # transform random samples\n",
    "    rand_features_single_class=scaler.transform(rand_features_single_class.iloc[:,1:-1])\n",
    "    # find optimal clustering\n",
    "    n_cluster_optimal,kmeans_model_optimal,labels_optimal=find_clusters_KMeans(rand_features_single_class,min_cluster=3,max_cluster=20)\n",
    "\n",
    "    # normalisation before clustering\n",
    "    model_input=scaler.transform(td_single_class.iloc[:,1:-3])\n",
    "    # predict clustering labels\n",
    "    labels_kmeans = kmeans_model_optimal.predict(model_input)\n",
    "    # append clustering results to pixel coordinates\n",
    "    td_single_class['cluster']=labels_kmeans\n",
    "    # append frequency of each cluster\n",
    "    cluster_frequency=td_single_class['cluster'].map(td_single_class['cluster'].value_counts(normalize=True))\n",
    "    td_single_class['cluster_frequency']=cluster_frequency\n",
    "    # filter by cluster frequency\n",
    "    td_single_class_filtered=td_single_class[td_single_class['cluster_frequency']>=frequency_threshold]\n",
    "    print('Number of training data after filtering: ',len(td_single_class_filtered))\n",
    "    \n",
    "    # append the filtered training points of this class to final filtered training data\n",
    "    if training_features_filtered is None:\n",
    "        training_features_filtered=td_single_class_filtered\n",
    "    else:\n",
    "        training_features_filtered=pd.concat([training_features_filtered, td_single_class_filtered])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffedd3d-6817-4c3a-aca6-d5b5d98f85ad",
   "metadata": {},
   "source": [
    "## Export filtered training features\n",
    "Once we've filtered the training signatures, we can write the filtered data to disk, which will allow us to import the data in the next step(s) of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b97829f-3578-4ce3-95e6-ba2be2b99820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the filtered training data as geojson file\n",
    "output_file = \"Results/Mozambique_training_features_filtered.geojson\"\n",
    "training_features_filtered.to_file(output_file, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed675c5-4dc6-4f71-850f-55e0c685334f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "01a9736267bf300689e8e016092cd01f0c67384dd94651ae6e139a291bc8cc97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
