{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924c9dca-2e38-41f9-a495-a780652af2e0",
   "metadata": {},
   "source": [
    "# Extract Training Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf04baac-d23d-4b9a-a64e-20331dc2fd98",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Background\n",
    "\n",
    "**Training data** is the most important part of any supervised machine learning workflow. The quality of the training data has a greater impact on the classification than the algorithm used. Large and accurate training data sets are preferable: increasing the training sample size results in increased classification accuracy ([Maxell et al 2018](https://www.tandfonline.com/doi/full/10.1080/01431161.2018.1433343)).  A review of training data methods in the context of Earth Observation is available [here](https://www.mdpi.com/2072-4292/12/6/1034) \n",
    "\n",
    "There are many platforms to use for gathering land cover training labels, the best one to use depends on your application. GIS platforms are great for collection training data as they are highly flexible and mature platforms; [Geo-Wiki](https://www.geo-wiki.org/) and [Collect Earth Online](https://collect.earth/home) are two open-source websites that may also be useful depending on the reference data strategy employed. Alternatively, there are many pre-existing training datasets on the web that may be useful, e.g. [Radiant Earth](https://www.radiant.earth/) manages a growing number of reference datasets for use by anyone. With locations of land cover labels available, we can extract features at these locations from satellite imagery as input for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273bf0c5-e061-46ce-b127-775480319eaf",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook will extract training feature layers from the `open-data-cube` of Sentinel-2 multispectral images using geometries within a geojson.\n",
    "\n",
    "To do this, we rely on a custom function called `collect_training_data`, contained within the [deafrica_tools.classification](../../../../Tools/deafrica_tools/classification.py) script.  The principal goal of this notebook is to familarise users with this function so they can extract the appropriate data for their use-case. The default example also highlights extracting a set of useful feature layers for land cover classification. The workflow includes the following steps:\n",
    "\n",
    "1. Preview the points in our training data by plotting them on a basemap\n",
    "2. Define a feature layer function to pass to `collect_training_data`\n",
    "3. Extract training features from the datacube using `collect_training_data`\n",
    "4. Export the training data to disk for use in subsequent scripts\n",
    "\n",
    "***\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2ed76c-c7be-4867-8a87-be94069fed16",
   "metadata": {},
   "source": [
    "### Load packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed71faf-13df-4e20-a384-a115d19e8baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import datacube\n",
    "import warnings\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from odc.algo import xr_geomedian\n",
    "from deafrica_tools.plotting import map_shapefile\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.bandindices import calculate_indices\n",
    "from deafrica_tools.classification import collect_training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7a023c-1879-4664-950e-4d4e7a88b7ea",
   "metadata": {},
   "source": [
    "## Analysis parameters\n",
    "\n",
    "* `training_data_path`: The path to the input vector file from which we will extract training data. A default geojson is provided for this notebook.\n",
    "* `class_attr`: This is the name of column in your shapefile attribute table that contains the class labels. **The class labels must be integers**\n",
    ">**Note**: If you wish to run your own classification workflow, the first step is to replace this training data with your own in the '/Data' folder. The training data can be in other formats (e.g. shapefile) that can be read by `geopandas` but need to have a corresponding class label attribute field `class_attr`.\n",
    "* `output_crs`: Output spatial reference system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0859019-9d67-49d8-a667-061c359432ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_path = 'Data/train_poly_848_20171124_class_merged_buffered_smaller.geojson'\n",
    "class_attr = 'LC_Class_I' # class label in integer format\n",
    "output_crs='epsg:32736' # WGS84/UTM Zone 36S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f2fc0f-3368-40f4-938e-5d718938f06d",
   "metadata": {},
   "source": [
    "## Load input data\n",
    "\n",
    "We can load and preview our input data shapefile using `geopandas`. The shapefile should contain a column with class labels (e.g. 'class_attr') which must be integer type. These labels will be used to train our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bcec2a-820c-4902-b8f7-d91a0b9f3fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input data shapefile\n",
    "training_data= gpd.read_file(training_data_path) # read training points as geopandas dataframe\n",
    "training_data=training_data[[class_attr,'geometry']] # select attributes\n",
    "# Plot first five rows\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50868cf-f8df-4b40-a81a-e2bdf61bbf86",
   "metadata": {},
   "source": [
    "Now let's plot some training data in an interactive map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace44a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1:'Forest',5:'Grassland',7:'Shrubland',9:'Perennial Cropland',10:'Annual Cropland',11:'Wetland',12:'Water Body',13:'Urban Settlement'\n",
    "# Satellite hybrid: https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}\n",
    "# Satellite: https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}\n",
    "# map_shapefile(training_data[0:10], attribute=class_attr,continuous=False, hover_col=True)\n",
    "training_data_subset = training_data[0:10]\n",
    "\n",
    "training_data_subset.explore(\n",
    "    tiles = \"https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}\", \n",
    "    attr ='Imagery @2022 Landsat/Copernicus, Map data @2022 Google',\n",
    "    popup=True,\n",
    "    cmap='viridis',\n",
    "    style_kwds=dict(radius= 5, color= 'red', fillOpacity= 0.8, fillColor= 'red', weight= 3),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07be3ae0",
   "metadata": {},
   "source": [
    "This interactive map plots the first ten records from our input vector file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7458b0d-7391-4d8a-83f7-4f183120a4a2",
   "metadata": {},
   "source": [
    "## Defining Query\n",
    "\n",
    "The function `collect_training_data` takes our geojson containing class labels and extracts training data (features) from the datacube over the locations specified by the input geometries. The function will also pre-process our training data by stacking the arrays into a useful format and removing any `NaN` or `inf` values.\n",
    "\n",
    "The below variables can be set within the `collect_training_data` function:\n",
    "\n",
    "* `feild`: The name of column in your geojson file attribute table that contains the class labels, which corresponds to the `class_attr` that we defined earlier.\n",
    "* `zonal_stats`: An optional string giving the names of zonal statistics to calculate across each geometry (polygon or point). Default is None (all pixel values are returned). Supported values are 'mean', 'median', 'max', and 'min'.\n",
    "* `dc_query`: A datacube query dictionary for the Open Data Cube query such as `measurements` (the bands to load from the satellite), the `resolution` (the cell size), and the `output_crs` (the output projection). \n",
    "* `feature_func`:  A function for generating feature layers that is applied to the data within the bounds of the input geometry. This function will take the 'dc_query' as the only argument.\n",
    "* `return_coords`: If True, then the training data will contain two extra columns ‘x_coord’ and ‘y_coord’ corresponding to the x,y coordinate of each sample.\n",
    "\n",
    "> Note: `collect_training_data` also has a number of additional parameters for handling ODC I/O read failures, where polygons that return an excessive number of null values can be resubmitted to the multiprocessing queue.  Check out the [docs](https://github.com/digitalearthafrica/deafrica-sandbox-notebooks/blob/83116e80ebb4f8744e3de74e7a713aadd0a7577a/Tools/deafrica_tools/classification.py#L565) to learn more.\n",
    "\n",
    "we will define the first three parameters and describe the `feature_func` seperately in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c428fd-6aff-44c5-9c92-b01d5eaccbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up our inputs to collect_training_data\n",
    "zonal_stats = None\n",
    "# Set up the inputs for the ODC query\n",
    "time = ('2021')\n",
    "# using nine spectral bands with 10~20 m spatial resolution\n",
    "measurements = ['blue','green','red','red_edge_1','red_edge_2', 'red_edge_3','nir_1','swir_1','swir_2']\n",
    "resolution = (-10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738be341-371b-4497-85bb-3ca528302340",
   "metadata": {},
   "source": [
    "Note that we've selected nine spectral bands with spatial resolution no lower than 20 m here for demonstration. However, it is advised that you test and select the bands based on your own classification task. Using the variables above, we can generate a datacube query object from the parameters above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21decf8f-5b13-4a93-9403-0ccdbf7f1e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    'time': time,\n",
    "    'measurements': measurements,\n",
    "    'output_crs': output_crs,\n",
    "    'resolution': resolution\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0a2bc0-af97-4e46-89b6-4f3dffff7ce3",
   "metadata": {},
   "source": [
    "## Defining feature function\n",
    "\n",
    "To create the desired feature layers, we pass instructions to `collect_training_data` through the `feature_func` parameter. The `feature_func` must accept a `dc_query` dictionary, and return a single `xarray.Dataset` or `xarray.DataArray` containing 2D coordinates (i.e x, y - no time dimension). e.g.\n",
    "\n",
    "          def feature_function(query):\n",
    "              dc = datacube.Datacube(app='feature_layers')\n",
    "              ds = dc.load(**query)\n",
    "              ds = ds.mean('time')\n",
    "              return ds\n",
    "\n",
    "Below, we will define a more complicated feature layer function than the brief example shown above. Firstly We will calculate the Normalised Difference Vegetation Index (NDVI), which is commonly used to distinguish between vegetation and non-vegetation land cover classes. We use the `calculate_indices`function to automatically calculate NDVI for all specified bands. The `calculate_indicies` function provides an easier way to calculate a wide range of remote sensing indices, without explicitly needing to write the code of the band math. More information on calculating band indices can be found in [DE Africa’s documentation](https://docs.digitalearthafrica.org/en/latest/sandbox/notebooks/Frequently_used_code/Calculating_band_indices.html). \n",
    "\n",
    "\n",
    "In addition, we'll use temporal signatures to help distinguish land cover classes with seasonal patterns (e.g. croplands) with others. To reduce data size while keeping seasonal changes, we are implementing bimonthly temporal aggregation, i.e. geomedian (sometimes referred to as the 'geometric median') for each pixel location. The significance of the geomedian is that all bands are considered simultaneously. This maintains the spectral relationship between bands, providing the most representative value. Additionally, the geomedian statistic reduces spatial noise and improves colour balance compared to similar statistics such as the median or medoid. More information on geomedian and available geomedian products can be found in [DE Africa's documentation](https://docs.digitalearthafrica.org/en/latest/data_specs/GeoMAD_specs.html).\n",
    "> Note: A shorter time interval could be chosen to obtain a finer-grained temporal signatures, but it would also result in larger data gaps in the satellite time-series due to presence of cloud and cloud shadow. If cloud presence in your study site is very frequent, you may need to choose a courser temporal aggregates, e.g. every three months.\n",
    "\n",
    "As the output of the feature layer function should only contain coordinates with no time dimension, we need to convert the temporal features of each measurement to multiple individual features. For example, we will rename and convert the bimonthly temporal features for blue band to blue_0, blue_2, ..., blue_5.\n",
    "\n",
    "Putting all above together, our final feature layer function looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb83d25-3307-4b7c-9001-2577b5137bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to feature layers\n",
    "def feature_layers(query): \n",
    "    # connect to the datacube so we can access DE Africa data\n",
    "    dc = datacube.Datacube(app='feature_layers')\n",
    "    \n",
    "    # load Sentinel-2 analysis ready data\n",
    "    ds = load_ard(dc=dc,\n",
    "                  products=['s2_l2a'],\n",
    "                  group_by='solar_day',\n",
    "                  verbose=False,\n",
    "                  **query)\n",
    "    \n",
    "    # calculate NDVI\n",
    "    ds = calculate_indices(ds,\n",
    "                           index=['NDVI'],\n",
    "                           drop=False,\n",
    "                           satellite_mission='s2')\n",
    "    \n",
    "    # interpolate nodata using mean of previous and next observation\n",
    "#     ds=ds.interpolate_na(dim='time',method='linear',use_coordinate=False,fill_value='extrapolate')\n",
    "#     ds=ds.interpolate_na(dim='time',method='linear',use_coordinate=False)\n",
    "\n",
    "    # calculate bi-monthly geomedian\n",
    "    ds=ds.resample(time='2MS').map(xr_geomedian)\n",
    "    \n",
    "    # stack multi-temporal measurements and rename them\n",
    "    n_time=ds.dims['time']\n",
    "    list_measurements=list(ds.keys())\n",
    "    list_stack_measures=[]\n",
    "    for j in range(len(list_measurements)):\n",
    "        for k in range(n_time):\n",
    "            variable_name=list_measurements[j]+'_'+str(k)\n",
    "            measure_single=ds[list_measurements[j]].isel(time=k).rename(variable_name)\n",
    "            list_stack_measures.append(measure_single)\n",
    "    ds_stacked=xr.merge(list_stack_measures,compat='override')\n",
    "    return ds_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8631bc-59b6-45e6-87a1-173fa1c6b158",
   "metadata": {},
   "source": [
    "Now let's run the `collect_training_data` function. This may take minutes to hours depending on your number of training points, number of measurements/bands set for the query and the calculation work in the feature function. Since we've used 10 measurements (9 spectral bands and 1 NDVI index) with 6 temporal geomedian for each band, it can be very time-consuming to finish the training features extraction. Therefore, here we are passing in `gdf=training_data[0:10]` to only run the code over the first 10 geometries as demonstration. Nevertheless, the extracted full training data file is provided in the 'Results/' folder, which will be used for next module of the workflow.\n",
    "\n",
    "> **Note**:  With supervised classification, its common to have many, many labelled geometries in the training data. `collect_training_data` can parallelize across the geometries in order to speed up the extracting of training data. Setting `ncpus>1` will automatically trigger the parallelization. However, its best to set `ncpus=1` to begin with to assist with debugging before triggering the parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0c4b27-6a92-4823-85c0-f296fbc72b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect the number of CPUs\n",
    "ncpus=round(get_cpu_quota())\n",
    "print('ncpus = '+str(ncpus))\n",
    "\n",
    "# collect training data\n",
    "column_names, model_input = collect_training_data(\n",
    "    gdf=training_data[0:10], # replace with gdf=training_data if you are extracting all the training data\n",
    "    dc_query=query,\n",
    "    ncpus=ncpus,\n",
    "    field=class_attr,\n",
    "    zonal_stats=zonal_stats,\n",
    "    feature_func=feature_layers,\n",
    "    return_coords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89da5489-a22e-47c6-b85f-5faa83b2fac0",
   "metadata": {},
   "source": [
    "The function returns a list (`column_names`) contains a list of the names of the `class_attr` and all the features we've computed. As we set the parameter `return_coords` above as True the list also contains the coordinates of the points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0735ba9b-d7ce-4be0-aa97-0fbb5411abda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7499eb-1802-4005-8659-fa4a0d94c84a",
   "metadata": {},
   "source": [
    "The second object returned by the function is a numpy.array (`model_input`) and contains the data from our labelled geometries. The first item in the array is the class integer (e.g. in the default example 61. 'Bare soils', 11. 'Tree crops', etc.), the second set of items are the values for each feature layer we computed and the coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce41957-68c7-45e3-bd44-8a0531d2bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array_str(model_input, precision=2, suppress_small=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb734ba3-c32c-489d-892e-b3c2573a19e1",
   "metadata": {},
   "source": [
    "## Export training features\n",
    "\n",
    "Once we've collected the training features we require, we can write the data to disk. For example to export the extracted first 10 training features as a text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701e6daf-1712-4a3b-af49-7c46693d06c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to geopandas dataframe\n",
    "pd_training_features=pd.DataFrame(data=model_input,columns=column_names)\n",
    "#set the name and location of the output file\n",
    "output_file = \"Results/Mozambique_training_features_demo.txt\"\n",
    "#Export files to disk\n",
    "pd_training_features.to_csv(output_file, header=True, index=None, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695bddd7",
   "metadata": {},
   "source": [
    "We would also like to allow us to check the locations of training points after training data filtering implemented through next notebook. To do that, we reconstruct a geopandas dataframe using the coordinates returned by the `collect_training_data` function and export the features as a geojson file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4a7ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create geopandas dataframe\n",
    "gpd_training_features=gpd.GeoDataFrame(pd_training_features, \n",
    "geometry=gpd.points_from_xy(model_input[:,-2], model_input[:,-1],crs=output_crs))\n",
    "# save as geojson file\n",
    "output_file='Results/Mozambique_training_features_demo.geojson'\n",
    "gpd_training_features.to_file(output_file, driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5b0778",
   "metadata": {},
   "source": [
    "The extracted full training features file is provided as 'Results/Mozambique_training_features.geojson', which will allow us to import the data in the next module(s) of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc85898-b9d2-4117-95dd-3576f4942e75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "01a9736267bf300689e8e016092cd01f0c67384dd94651ae6e139a291bc8cc97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
