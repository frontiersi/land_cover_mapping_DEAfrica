{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e34a93-c6d5-443f-9682-460058c99243",
   "metadata": {},
   "source": [
    "# Fit, Optimize and Evaluate A Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2166e46-c953-47f0-a82b-4b5bc68b5b88",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Now that we have training features extracted (and filtered if needed), we can train a machine learning model. There are various classification models available, and deciding which one to pick depends on the classification task at-hand. The table below provides a useful summary of the pros and cons of different models (all of which are available through [scikit-Learn](https://scikit-learn.org/stable/)). This sckit-learn [cheat sheet](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) may also help.  \n",
    "_Table 1: Some of the pros and cons of different classifiers available through scikit-learn_\n",
    "<img align=\"center\" src=\"../../../../../Supplementary_data/Scalable_machine_learning/classifier_pro_cons.png\" width=\"700\">\n",
    "\n",
    "\n",
    "In this example we are using a Random Forest classifier for the land cover classification task. We'll split the data into a training and testing set and use k-fold cross-validation to estimate the prediction ability of our model. To make full use of the limited number of training dara, here we will fit the final model on _all_ the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7120659-d2bf-4c4d-a5fc-1ee6f290888c",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "This notebook runs through evaluating, optimizing, and fitting a machine learning classifier (in the default example, a Random Forest model is used). The steps are as follows:\n",
    "\n",
    "1. Import training features\n",
    "2. Optimize the hyperparameters of the model\n",
    "3. Fit a model to all the training data using the best hyperparameters identified in the previous step\n",
    "4. Calculate an unbiased performance estimate via cross-validation\n",
    "5. Save the model to disk for use in the subsequent notebook\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5ce54e-f95a-4226-8162-e3c135d56200",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9fb1dd-deff-4e08-8710-42b0ccb7e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import datacube\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score,cohen_kappa_score,confusion_matrix,ConfusionMatrixDisplay,balanced_accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3234bf9e-915b-4b58-aa95-edadc01c660d",
   "metadata": {},
   "source": [
    "## Analysis parameters\n",
    "\n",
    "* `training_features_path`: The path to the file containing training features we extracted through previous module `0_Extract_Training_Features.ipynb` or filtered through `1_Filter_Training_Data.ipynb`.\n",
    "* `dict_map` : A dictionary map of class names corresponding to class/pixel values.\n",
    "* `class_attr`: This is the name of column in your shapefile/geojson file attribute table that contains the class labels. **The class labels must be integers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb004d94-5b7b-40e0-a217-fda2633629f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features_path='Results/Mozambique_training_features_filtered.txt'\n",
    "dict_map={11:'Tree crops',12:'Field crops',21:'Forest plantations',31:'Grassland',\n",
    "                 41:'Aquatic or regularly flooded herbaceous vegetation',44:'Water body',\n",
    "                 51:'Settlements',61:'Bare soils',70:'Mangrove',71:'Mecrusse',\n",
    "                72:'Broadleaved (Semi-) evergreen forest',74:'Broadleaved (Semi-) deciduous forest',75:'Mopane'} # dictionary of class names corresponding to pixel values\n",
    "class_attr = 'Class_I' # class label in integer format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfe6528-7830-485d-aeef-4a57c983160f",
   "metadata": {},
   "source": [
    "## Import training data\n",
    "We now load the training features file. The dataframe should contain columns `class_attr` identifying class labels and the bi-monthly geomedians of the ten spectral bands and NDVI that we extracted through previous module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46219172-f902-47a3-8884-fe689790f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_data= pd.read_csv(training_features_path,sep=\" \") # Load training features\n",
    "df_training_data.head() # Plot first five rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605d79a2-2c59-4805-ba84-6a6e2e2ff898",
   "metadata": {},
   "source": [
    "To check the number of samples for each class, we can plot a histogram by class. Note that the training features only contain integer class values, we retrive the class names using the `dict_map`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb38112-c17b-43a0-95a0-46685cb2242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count numbers of classes\n",
    "class_counts=df_training_data[class_attr].value_counts()\n",
    "\n",
    "# get class names\n",
    "class_indices=class_counts.index\n",
    "class_legends=[dict_map[class_indices[i]] for i in range(len(class_indices))]\n",
    "\n",
    "# plot training samples distribution\n",
    "plt.figure(figsize=(15,5))\n",
    "ax=plt.bar(class_legends,height=class_counts.to_numpy())\n",
    "plt.bar_label(ax)\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.gca().set_ylabel('Number of Training Samples')\n",
    "plt.gca().set_xlabel('Land Cover Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04784a96-dbca-40af-9bc3-4098ede9536c",
   "metadata": {},
   "source": [
    "> Note: One of the common problems with training data is **class imbalance**. This can occur when one of your classes is relatively rare and therefore the rare class will comprise a smaller proportion of the training set. When imbalanced data is used, it is common that the final classification will under-predict less abundant classes relative to their true proportion. If you think your data is imbalanced, you may consider adding more training samples for under-represented classes or removing samples for over-represented classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe49424f-bd97-4755-87ba-de9980b2cf20",
   "metadata": {},
   "source": [
    "## Optimize hyperparameters\n",
    "\n",
    "Machine learning models require certain 'hyperparameters': model parameters that can be tuned to increase the prediction ability of a model. Finding the best values for these parameters is a 'hyperparameter search' or an 'hyperparameter optimization'.\n",
    "\n",
    "To optimize the parameters in our model, we use [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to exhaustively search through a set of parameters and determine the combination that will result in the highest accuracy based upon the accuracy metric defined. We first need to define a grid of parameters to be used in the optimization:\n",
    "* `grid_parameters`: a dictionary of model specific parameters to search through during hyperparameter optimization.\n",
    "\n",
    "> **Note**: the parameters in the `grid_parameters` object depend on the classifier being used. The default example is set up for a Random Forest classifier. To adjust the parameters to suit a different classifier, look up the important parameters under the relevant [sklearn documentation](https://scikit-learn.org/stable/supervised_learning.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d39ef-e596-4e75-b9d6-05731866ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "grid_parameters={'n_estimators': [int(x) for x in np.linspace(start = 100, stop = 200, num = 3)],\n",
    "                 'max_features': ['sqrt'],'max_samples':[0.5]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caafbd4-22a1-4d37-aeb1-a4aec7c4fe1c",
   "metadata": {},
   "source": [
    "We now start searching for the optimal hyperparameters using `GridSearchCV`. It may take a while depending on your searching space. Here we use k-fold splitting strategy for the search, which we will cover later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cdc57f-fa47-4905-b000-a061f6383114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise a random forest classifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# stratified K-fold splitting strategy for grid search\n",
    "cv=model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=0) \n",
    "\n",
    "# grid search to find optimal random forest classifier hyperparameters\n",
    "print('Grid searching best hyper-parameters...')\n",
    "grid_search=model_selection.GridSearchCV(estimator = rf,param_grid=grid_parameters,cv=cv,n_jobs=-1)\n",
    "\n",
    "#convert variable names into sci-kit learn nomenclature\n",
    "X = df_training_data.to_numpy()[:,1:]\n",
    "y = df_training_data.to_numpy()[:,0]\n",
    "\n",
    "# fit to all the data\n",
    "grid_search.fit(X,y)\n",
    "print('Optimal parameters: \\n',grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e43f41-4533-4521-b957-28f1f8b45ccf",
   "metadata": {},
   "source": [
    "## Fit the model\n",
    "\n",
    "Using the best parameters from our hyperparmeter optimization search, we now fit our model on all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463edee9-36f7-49ea-b413-d812f8b7da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(**grid_search.best_params_, random_state=1, n_jobs=-1)\n",
    "rf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c71b7-6e5e-4937-bd4f-122b7ff636e3",
   "metadata": {},
   "source": [
    "## Determine feature importance\n",
    "\n",
    "Here we extract classifier estimates of the relative importance of each feature for training a Random Forest classifier. This is useful for potentially selecting a subset of input bands/variables for model training/classification (i.e. optimising feature space). However, in this case, we are not selecting a subset of features, but rather just trying to understand the importance of each feature. This can help us not only understand our classes better (e.g. what kinds of measurements are more important in predicting the land cover classes), but can also lead to further improvements to the model. \n",
    "\n",
    "Sklearn has good documentation on different methods for [feature selection](https://scikit-learn.org/stable/modules/feature_selection.html).\n",
    "\n",
    "Results will be presented in ascending order with the most important features listed last. Importance is reported as a relative fraction between 0 and 1. These importances are based on how much a given feature, on average, decreases the weighted Gini impurity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e338be-59f8-4070-9b23-447818b9a1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "plt.figure(figsize=(30,5))\n",
    "order=np.argsort(rf.feature_importances_)\n",
    "plt.bar(x=np.array(df_training_data.columns[1:])[order],height=rf.feature_importances_[order],width=0.5,align='center')\n",
    "plt.xticks(rotation='vertical',fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xlabel('Variable', fontsize=20)\n",
    "plt.ylabel('Importance', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4ae500-0d30-41fa-ac60-445a8afbb53b",
   "metadata": {},
   "source": [
    "Observe the above figure and get an idea of what bands/features are most important for discriminating different classes in our selected area. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91dc42f-e417-4451-80fc-da456eb167dd",
   "metadata": {},
   "source": [
    "## Evaluate performance via cross-validation\n",
    "\n",
    "K-fold [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) is a statistical method used to estimate the performance of machine learning models when making predictions on data not used during training.  It is a popular method because it is conceptually straightforward and because it generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train/test split. This procedure can be used both when optimizing the hyperparameters of a model on a dataset, and when comparing and selecting a model for the dataset. \n",
    "\n",
    "Since our training dataset is small and to make sure each class has samples in the training set, in this example the `StratifiedKFold` splitting is used. Stratified k-fold is a variation of k-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set. We define 5 folds splitting, which divides the dataset into 5 sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc0885-66b5-486e-b692-203f082736c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified K-fold splitting\n",
    "skf=model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffe18d0-9c39-4258-907c-b71ba7a69c69",
   "metadata": {},
   "source": [
    "We now use `cross_val_score` function which calculates an accuracy score that show how well our classifier is doing at recognising points in each test split through cross validation. The default example is set up to calculate the `accuracy_score` or Overall Accuracy (OA), which is the fraction of correct predictions for all the samples. The OA will be between 0 and 1, with a value of 1 indicating a perfect score. There are other scoring paramters that you can choose [here](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d4dfcc-8738-47d5-9674-23d3a87c34ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate overall accurcy through cross validation\n",
    "overall_acc=model_selection.cross_val_score(rf,X,y,cv=skf,scoring='accuracy')\n",
    "print('Overall accuracy from cross validation: ',np.mean(overall_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f106ba3-6460-4fe9-9f21-17a44e77cc54",
   "metadata": {},
   "source": [
    "To show how the prediction performs on each class, the example will calculate the predictions from cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0af456f-6e1d-4177-91bc-2eac25a8a69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict through cross validation\n",
    "predictions=model_selection.cross_val_predict(rf,X,y,cv=skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad36c554-43bb-4a23-886d-a12c87d6f5b8",
   "metadata": {},
   "source": [
    "Using the predicted labels, we can calculate class-wise scores, such as precision and recall. The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The best value is 1 and the worst value is 0. We use the function `precision_score` for the calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66cc22c-cc45-420f-9d43-dfc87ecbaa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision=precision_score(y,predictions,labels=list(dict_map.keys()),average=None)\n",
    "print('Precision for each class: \\n',dict(zip([dict_map[value] for value in list(dict_map.keys())],np.around(precision,3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42838d41-7094-4f7d-9e98-55fa51315a04",
   "metadata": {},
   "source": [
    "Similarly we can calculate the recall scores using the function `recall_score`. The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ab939-7004-49e6-b284-6caa0f40ae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall=recall_score(y,predictions,labels=list(dict_map.keys()),average=None)\n",
    "print('Recall for each class: \\n',dict(zip([dict_map[value] for value in list(dict_map.keys())],np.around(precision,3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d833729-e7d5-4f08-b9dc-ce1c50e40263",
   "metadata": {},
   "source": [
    "To further inspect the actual numbers of predictions for each class we calculate and plot the `confusion_matrix`. By definition, entry *i, j* in a confusion matrix is the number of observations actually in group *i*, but predicted to be in group *j*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db755bb8-0bb0-4a54-b34a-7b598c7715f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate and visualise confusion matrix\n",
    "cm=confusion_matrix(y, predictions)\n",
    "display_labels=[dict_map[rf.classes_[i]] for i in range(len(rf.classes_))]\n",
    "disp=ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=display_labels)\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "disp.plot(ax=ax)\n",
    "ax.set_title('Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebcc39f-684d-4b27-b681-b1fd095a9c8a",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "We now export the trained classifier as a binary`.joblib` file, which will allow for importing the model in the subsequent module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae663572-828d-4f81-af54-ceb26817589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(rf, 'Results/Mozambique_RF_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1136a02-19bb-409c-84e9-58e4cec723ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "01a9736267bf300689e8e016092cd01f0c67384dd94651ae6e139a291bc8cc97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
