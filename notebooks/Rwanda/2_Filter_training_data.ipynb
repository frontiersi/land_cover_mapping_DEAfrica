{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements training data filtering using kmeans method. The filtered training data will then be used to train a classifier and produce a land cover classification map in 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import datacube\n",
    "import warnings\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from datacube.utils.cog import write_cog\n",
    "from odc.algo import xr_geomedian\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.bandindices import calculate_indices\n",
    "from deafrica_tools.classification import collect_training_data\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from rasterio.enums import Resampling\n",
    "from random_sampling import random_sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameters and file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths and attributes\n",
    "training_signature_path='Results/Training_features_Rwanda.txt'\n",
    "\n",
    "rf2017_path='Results/rwanda_landcover_2015_scheme_ii_classes_merged.tif'\n",
    "\n",
    "crs='epsg:32735' # WGS84/UTM Zone 35S\n",
    "measurements = ['blue','green','red','red_edge_1','red_edge_2', 'red_edge_3','nir_1','nir_2','swir_1','swir_2','NDVI']\n",
    "class_attr = 'LC_Class_I' # class label in integer format\n",
    "column_names=[class_attr]\n",
    "for measurement in measurements:\n",
    "    for i in range(6):\n",
    "        column_names.append(measurement+'_'+str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load training data, tiles and reference map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "land cover survey points 2017:\n",
      "       LC_Class_I       blue_0       blue_1       blue_2       blue_3  \\\n",
      "0              1   721.733276  6086.884766   738.500000   335.000000   \n",
      "1              1   760.000000   397.000000   355.338196   311.243500   \n",
      "2              1   459.827759   574.958130   268.073669   398.022430   \n",
      "3              1  1311.850342   842.000000   655.000061   592.346008   \n",
      "4              1   732.000000   914.500000   266.695496   199.000610   \n",
      "...          ...          ...          ...          ...          ...   \n",
      "8995          14   600.026733  6376.000000   739.782837   813.462280   \n",
      "8996          14  1395.889282  1056.000000  1115.973267  1080.053589   \n",
      "8997          14   768.067322   895.500000   821.279907   845.342834   \n",
      "8998          14  1324.946777  6872.000000   948.868042  1117.055420   \n",
      "8999          14  1454.510742  1188.000000   840.384583   872.758484   \n",
      "\n",
      "           blue_4       blue_5      green_0      green_1      green_2  ...  \\\n",
      "0      565.067627   379.737549   800.302795  5644.144531   846.000000  ...   \n",
      "1      859.999634   321.500000  1070.000000   727.500000   471.977051  ...   \n",
      "2      460.286102   764.000000   585.927063   647.710510   368.184448  ...   \n",
      "3      548.502380   440.219513  1443.244873  1080.000000   890.000061  ...   \n",
      "4     1188.326172   200.000000   800.000000   939.000000   363.970856  ...   \n",
      "...           ...          ...          ...          ...          ...  ...   \n",
      "8995   940.799988   683.500000   934.013672  6428.000000  1088.640625  ...   \n",
      "8996  1032.978149  1368.000000  1601.418579  1490.000000  1590.902100  ...   \n",
      "8997  1026.161255   672.000244  1243.466919  1401.000000  1353.263794  ...   \n",
      "8998  1395.754028   789.326721  1633.982422  6531.000000  1323.890625  ...   \n",
      "8999  1121.877808   737.000061  1844.860474  1448.000122  1085.219727  ...   \n",
      "\n",
      "         swir_2_2     swir_2_3     swir_2_4     swir_2_5    NDVI_0    NDVI_1  \\\n",
      "0     1258.000000  1033.000000   966.018799   524.931702  0.574543  0.033943   \n",
      "1      595.906128   723.668640   905.000000   759.500000  0.606968  0.611521   \n",
      "2      617.888916   603.654236   722.252625  1087.000000  0.743897  0.705448   \n",
      "3      952.000122   963.048767  1160.815918   942.825623  0.399115  0.489766   \n",
      "4      643.158447   499.999969   994.820435   572.000000  0.481399  0.580043   \n",
      "...           ...          ...          ...          ...       ...       ...   \n",
      "8995  1701.117554  2166.080078  1749.402344  1115.500000  0.619367  0.163669   \n",
      "8996  2395.471924  2585.630859  2472.122070  2234.000000  0.277664  0.366771   \n",
      "8997  2608.719238  2741.265869  2950.455078  2277.000000  0.357799  0.299483   \n",
      "8998  2267.153564  3142.516113  2759.847900  1757.821289  0.316653  0.184122   \n",
      "8999  2209.180908  2634.239502  2809.858398  1923.000000  0.262839  0.428191   \n",
      "\n",
      "        NDVI_2    NDVI_3    NDVI_4    NDVI_5  \n",
      "0     0.521250  0.613869  0.612500  0.686702  \n",
      "1     0.732555  0.801335  0.635953  0.782979  \n",
      "2     0.812925  0.728628  0.731337  0.599650  \n",
      "3     0.780302  0.712998  0.692139  0.761261  \n",
      "4     0.803988  0.797277  0.477547  0.855922  \n",
      "...        ...       ...       ...       ...  \n",
      "8995  0.521544  0.395901  0.321043  0.547166  \n",
      "8996  0.355593  0.317815  0.387408  0.344081  \n",
      "8997  0.209251  0.152240  0.116538  0.142706  \n",
      "8998  0.467995  0.274562  0.253911  0.514311  \n",
      "8999  0.499613  0.345723  0.292584  0.509498  \n",
      "\n",
      "[9000 rows x 67 columns]\n"
     ]
    }
   ],
   "source": [
    "training_features= pd.read_csv(training_signature_path,sep=' ')\n",
    "training_features=training_features[column_names] # select attributes\n",
    "training_features[class_attr]=training_features[class_attr].astype(int)\n",
    "print('land cover survey points 2017:\\n',training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference land cover classifcation raster:\n",
      " <xarray.DataArray 'band_data' (y: 6998, x: 7745)>\n",
      "array([[  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       ...,\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [  0,   0,   0, ...,   0,   0,   0],\n",
      "       [255, 255, 255, ..., 255, 255, 255]], dtype=uint8)\n",
      "Coordinates:\n",
      "  * x            (x) float64 7.044e+05 7.044e+05 ... 9.366e+05 9.367e+05\n",
      "  * y            (y) float64 9.887e+06 9.887e+06 ... 9.678e+06 9.678e+06\n",
      "    spatial_ref  int64 0\n",
      "Attributes:\n",
      "    AREA_OR_POINT:  Area\n",
      "    _FillValue:     255\n"
     ]
    }
   ],
   "source": [
    "reference_map = xr.open_dataset(rf2017_path,engine=\"rasterio\").astype(np.uint8).squeeze(\"band\", drop=True)\n",
    "# # reproject the raster\n",
    "reference_map= reference_map.rio.reproject(resolution=30, dst_crs=crs,resampling=Resampling.nearest)\n",
    "reference_map=reference_map.band_data\n",
    "print('Reference land cover classifcation raster:\\n',reference_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define queries and feature layer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    'time': ('2021-01', '2021-12'),\n",
    "    'output_crs': crs,\n",
    "    'resolution': (-10, 10)\n",
    "}\n",
    "# define a function to feature layers\n",
    "def feature_layers(query):\n",
    "    measurements = ['blue','green','red','red_edge_1','red_edge_2', 'red_edge_3','nir_1','nir_2','swir_1','swir_2']\n",
    "    #connect to the datacube\n",
    "    dc = datacube.Datacube(app='feature_layers')\n",
    "    ds = load_ard(dc=dc,\n",
    "                  products=['s2_l2a'],\n",
    "                  measurements=measurements,\n",
    "                  group_by='solar_day',\n",
    "                  verbose=False,\n",
    "#                   mask_filters=[(\"opening\", 2)], # morphological opening by 2 pixels to remove small masked regions\n",
    "                  **query)\n",
    "    ds = calculate_indices(ds,\n",
    "                           index=['NDVI'],\n",
    "                           drop=False,\n",
    "                           satellite_mission='s2')\n",
    "    # calculate geomedians within each two-month interval\n",
    "    ds=ds.resample(time='2MS').map(xr_geomedian)\n",
    "    # stack multi-temporal measurements and rename them\n",
    "    n_time=ds.dims['time']\n",
    "    list_measurements=list(ds.keys())\n",
    "    ds_stacked=None\n",
    "    for j in range(len(list_measurements)):\n",
    "        for k in range(n_time):\n",
    "            variable_name=list_measurements[j]+'_'+str(k)\n",
    "            measure_single=ds[list_measurements[j]].isel(time=k).rename(variable_name)\n",
    "            if ds_stacked is None:\n",
    "                ds_stacked=measure_single\n",
    "            else:\n",
    "                ds_stacked=xr.merge([ds_stacked,measure_single],compat='override')\n",
    "    return ds_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random samples - stratified by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "land cover classes:\n",
      " [ 1  5  7  9 10 11 12 13 14]\n"
     ]
    }
   ],
   "source": [
    "lc_classes=training_features[class_attr].unique() # get class labels\n",
    "n_class=len(lc_classes)\n",
    "print('land cover classes:\\n',lc_classes)\n",
    "n_samples=1000 # number of random samples per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1.0: sampling at 1000locations\n",
      "Class 5.0: sampling at 1000locations\n",
      "Class 7.0: sampling at 1000locations\n",
      "Class 9.0: sampling at 1000locations\n",
      "Class 10.0: sampling at 1000locations\n",
      "Class 11.0: sampling at 1000locations\n",
      "Class 12.0: sampling at 1000locations\n",
      "Class 13.0: sampling at 1000locations\n",
      "Class 14.0: sampling at 1000locations\n"
     ]
    }
   ],
   "source": [
    "reference_map=reference_map.where(reference_map!=255,0)\n",
    "n=1000\n",
    "random_samples_all=None\n",
    "for i in lc_classes:\n",
    "    da=reference_map.where(reference_map==i,np.nan)\n",
    "    random_samples=random_sampling(da=da,n=n,sampling='stratified_random',\n",
    "                                   min_sample_n=50,out_fname=None,class_attr=class_attr,drop_value=0)\n",
    "    random_samples[class_attr]=random_samples[class_attr].astype(int)\n",
    "    random_samples=random_samples.set_crs(reference_map.rio.crs)\n",
    "    if random_samples_all is None:\n",
    "        random_samples_all=random_samples\n",
    "    else:\n",
    "        random_samples_all=pd.concat([random_samples_all,random_samples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features for the random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect the number of CPUs\n",
    "ncpus=round(get_cpu_quota())\n",
    "print('ncpus = '+str(ncpus))\n",
    "# collect training data\n",
    "column_names, model_input = collect_training_data(\n",
    "    gdf=random_samples_all,\n",
    "    dc_query=query,\n",
    "    ncpus=ncpus,\n",
    "    field=class_attr,\n",
    "    zonal_stats=None,\n",
    "    feature_func=feature_layers,\n",
    "    return_coords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_samples_features=pd.DataFrame(data=model_input,columns=column_names)\n",
    "#set the name and location of the output file\n",
    "output_file = 'Results/Random_samples_features_Rwanda.txt'\n",
    "#Export files to disk\n",
    "rand_samples_features.to_csv(output_file, header=True, index=None, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clusters_KMeans(data,min_cluster=5,max_cluster=20):\n",
    "    highest_score=-999\n",
    "    n_cluster_optimal=min_cluster\n",
    "    kmeans_model_optimal=None # initialise optimal model parameters\n",
    "    labels_optimal=None\n",
    "    if min_cluster==max_cluster:\n",
    "        print('Implementing kmeans clustering with number of clusters: ',max_cluster)\n",
    "        kmeans_model_optimal = KMeans(n_clusters=max_cluster, random_state=1).fit(data)\n",
    "        labels_optimal=kmeans_model_optimal.predict(data)\n",
    "        n_cluster_optimal=max_cluster\n",
    "    else:\n",
    "        for n_cluster in range(min_cluster,max_cluster+1):\n",
    "            kmeans_model = KMeans(n_clusters=n_cluster, random_state=1).fit(data)\n",
    "            labels=kmeans_model.predict(data)\n",
    "            score=metrics.calinski_harabasz_score(data, labels)\n",
    "            print('Calinski-Harabasz score for ',n_cluster,' clusters is: ',score)\n",
    "            if (highest_score==-999)or(highest_score<score):\n",
    "                highest_score=score\n",
    "                n_cluster_optimal=n_cluster\n",
    "                kmeans_model_optimal=kmeans_model\n",
    "                labels_optimal=labels\n",
    "        print('Best number of clusters: %s'%(n_cluster_optimal))\n",
    "    return n_cluster_optimal,kmeans_model_optimal,labels_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features_filtered=None # filtered training data for all classes\n",
    "frequency_threshold=0.1 # threshold of cluter frequency\n",
    "optimal_clusters={1:None, 5: None, 7: None,  9: None, 10: None, 11: None,  12: None,  13: None,  14: None}\n",
    "scaler = StandardScaler() # standard scaler for input data standardisation\n",
    "for class_value in lc_classes: # filtering training data for each class\n",
    "    print('Processing class ',class_value)\n",
    "    rand_features_single_class=rand_samples_features[rand_samples_features[class_attr]==class_value].reset_index(drop=True)\n",
    "    np_rand_features=rand_features_single_class.to_numpy()[:,1:]\n",
    "    scaler.fit(np_rand_features)\n",
    "    np_rand_features=scaler.transform(np_rand_features)\n",
    "    if optimal_clusters[class_value] is None:\n",
    "        n_cluster_optimal,kmeans_model_optimal,labels_optimal=find_clusters_KMeans(np_rand_features,min_cluster=5,max_cluster=20)\n",
    "    else:\n",
    "        n_cluster_optimal,kmeans_model_optimal,labels_optimal=find_clusters_KMeans(np_rand_features,min_cluster=optimal_clusters[class_value],max_cluster=optimal_clusters[class_value])\n",
    "\n",
    "    # subset original training points for this class\n",
    "    td_single_class=training_features[training_features[class_attr]==class_value].reset_index(drop=True)\n",
    "    print('Number of training pints for the class: ',len(td_single_class))\n",
    "    np_td_single_class=td_single_class.to_numpy()[:,1:]\n",
    "    # predict clustering labels\n",
    "    np_td_single_class=scaler.transform(np_td_single_class)\n",
    "    labels_kmeans = kmeans_model_optimal.predict(np_td_single_class)\n",
    "    # append clustering results to pixel coordinates\n",
    "    td_single_class['cluster']=labels_kmeans\n",
    "\n",
    "    cluster_frequency=td_single_class['cluster'].map(td_single_class['cluster'].value_counts(normalize=True)) # calculate cluster frequencies for the training samples\n",
    "    td_single_class['cluster_frequency']=cluster_frequency # append as a column\n",
    "    td_single_class_filtered=td_single_class[td_single_class['cluster_frequency']>=frequency_threshold] # filter by cluster frequency\n",
    "    print('Number of training data after filtering: ',len(td_single_class_filtered))\n",
    "    \n",
    "    # append the filtered training points of this class to final filtered training data\n",
    "    if training_features_filtered is None:\n",
    "        training_features_filtered=td_single_class_filtered\n",
    "    else:\n",
    "        training_features_filtered=pd.concat([training_features_filtered, td_single_class_filtered])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export filtered training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"Results/Training_features_Rwanda_filtered.txt\"\n",
    "training_features_filtered.iloc[:,:-2].to_csv(output_file, header=True, index=None, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
