{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b95a70b-e46a-405a-9de7-7957efa6170a",
   "metadata": {},
   "source": [
    "This notebook implements extraction and filtering of training data on a per-class basis using a reference land cover map and k-means clustering. It incudes the following steps:\n",
    "1. Generate a set of randomly distributed samples for each class using the reference land cover map\n",
    "2. Extract features of the random samples\n",
    "3. Fit k-means clustering models using the features of the random samples\n",
    "4. Extract features for training points of each class\n",
    "5. Predict clustering labels of the training points\n",
    "6. Filter training points based on cluster frequency\n",
    "7. Merge filtered points for all classes and export filtered data \n",
    "\n",
    "The filtered training data will then be used to train a classifier and produce a land cover classification map in 2021."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "108ee53d",
   "metadata": {},
   "source": [
    "### load packages and get number of cpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fccc7e0-768c-4479-a777-6ce9efbf4d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import datacube\n",
    "import warnings\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from odc.algo import xr_geomedian\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.bandindices import calculate_indices\n",
    "from deafrica_tools.classification import collect_training_data\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from rasterio.enums import Resampling\n",
    "from random_sampling import random_sampling # adapted from function by Chad Burton: https://gist.github.com/cbur24/04760d645aa123a3b1817b07786e7d9f\n",
    "\n",
    "# get number of cpus\n",
    "ncpus=round(get_cpu_quota())\n",
    "print('ncpus = '+str(ncpus))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49751d11",
   "metadata": {},
   "source": [
    "### input files and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dc26d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths and attributes\n",
    "traning_points_path = 'Results/manual_number_random_training_points_scheme_ii_2015.geojson' # training points extracted from reference map\n",
    "ref_map_path='Results/rwanda_landcover_2015_scheme_ii_classes_merged.tif' # reference land cover map (class merged)\n",
    "class_name = 'LC_Class_I' # class label in integer format\n",
    "output_crs='epsg:32735' # WGS84/UTM Zone 35S\n",
    "\n",
    "# Load training points and reproject\n",
    "training_data_unfiltered= gpd.read_file(traning_points_path).to_crs(output_crs) # read training points as geopandas dataframe\n",
    "training_data_unfiltered=training_data_unfiltered[[class_name,'geometry']] # select attributes\n",
    "training_data_unfiltered[class_name]=training_data_unfiltered[class_name].astype(int)\n",
    "print('land cover training points (unfiltered):\\n',training_data_unfiltered)\n",
    "\n",
    "# further merge classes if needed\n",
    "# training_data_unfiltered.loc[training_data_unfiltered[class_name]==2,class_name]=1 # Open Forest (2) merged with Dense Forest (1) as Forest (1)\n",
    "# training_data_unfiltered.loc[training_data_unfiltered[class_name]==8,class_name]=6 # Wooded Grassland (8) merged with Open Grassland (6) as Grassland (6)\n",
    "\n",
    "# load reference classification map\n",
    "ref_map_raster = xr.open_dataset(ref_map_path,engine=\"rasterio\").astype(np.uint8).squeeze(\"band\", drop=True)\n",
    "# reproject the raster\n",
    "ref_map_raster= ref_map_raster.rio.reproject(resolution=10, dst_crs=output_crs,resampling=Resampling.nearest)\n",
    "print('Initial random forest classifcation raster:\\n',ref_map_raster) # note: 255 is nodata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca39a96c",
   "metadata": {},
   "source": [
    "### define query and feature layer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff1b5f8-3028-49a2-88cc-a136fc693d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ODC query\n",
    "measurements = ['blue','green','red','red_edge_1','red_edge_2', 'red_edge_3','nir_1','nir_2','swir_1','swir_2']\n",
    "query = {\n",
    "    'time': ('2021-01', '2021-12'),\n",
    "    'measurements': measurements,\n",
    "    'output_crs': output_crs,\n",
    "    'resolution': (-10, 10)\n",
    "}\n",
    "# define a function to feature layers\n",
    "def feature_layers(query): \n",
    "    \n",
    "    #connect to the datacube\n",
    "    dc = datacube.Datacube(app='feature_layers')\n",
    "    ds = load_ard(dc=dc,\n",
    "                  products=['s2_l2a'],\n",
    "                  group_by='solar_day',\n",
    "                  verbose=False,\n",
    "#                   mask_filters=[(\"opening\", 2)], # morphological opening by 2 pixels to remove small masked regions\n",
    "                  **query)\n",
    "    # calculate NDVI\n",
    "    ds = calculate_indices(ds,\n",
    "                           index=['NDVI'],\n",
    "                           drop=False,\n",
    "                           satellite_mission='s2')\n",
    "    \n",
    "    # calculate geomedians within each two-month interval\n",
    "    ds=ds.resample(time='2MS').map(xr_geomedian)\n",
    "    \n",
    "    # interpolate nodata using mean of previous and next observation\n",
    "#     ds=ds.interpolate_na(dim='time',method='linear',use_coordinate=False,fill_value='extrapolate')\n",
    "\n",
    "    # stack multi-temporal measurements and rename them\n",
    "    n_time=ds.dims['time']\n",
    "    list_measurements=list(ds.keys())\n",
    "    ds_stacked=None\n",
    "    for j in range(len(list_measurements)):\n",
    "        for k in range(n_time):\n",
    "            variable_name=list_measurements[j]+'_'+str(k)\n",
    "            # print ('Stacking band ',list_measurements[j],' at time ',k)\n",
    "            measure_single=ds[list_measurements[j]].isel(time=k).rename(variable_name)\n",
    "            if ds_stacked is None:\n",
    "                ds_stacked=measure_single\n",
    "            else:\n",
    "                ds_stacked=xr.merge([ds_stacked,measure_single],compat='override')\n",
    "    return ds_stacked"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2dd307e3",
   "metadata": {},
   "source": [
    "### per-class training data filter using k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd905a2-2137-4fb0-8c4b-3098c8c96b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters and variables for training data filtering\n",
    "lc_classes=training_data_unfiltered[class_name].unique() # class labels\n",
    "n_samples=1000 # number of random samples per class to train kmeans clusterer\n",
    "zonal_stats = None\n",
    "scaler = StandardScaler() # standard scaler for input data standardisation\n",
    "frequency_threshold=0.1 # threshold of cluter frequency\n",
    "td2021_filtered=None # initialise filtered training data\n",
    "\n",
    "# filtering training data for each class\n",
    "for i in lc_classes:\n",
    "    print('Processing class ',i)\n",
    "    \n",
    "    # generate randomly distributed samples to train and optimise a kmeans clusterer\n",
    "    da_mask=ref_map_raster.band_data # get data array\n",
    "    da_mask=da_mask.where(da_mask==i,np.nan) # replace other class values as nan so they won't be sampled\n",
    "    gpd_samples=random_sampling(da_mask,n_samples,sampling='stratified_random',manual_class_ratios=None,out_fname=None)\n",
    "    gpd_samples=gpd_samples.drop(columns=['spatial_ref','class']) # drop this attribute derived from random_sampling function\n",
    "    gpd_samples[class_name]=int(i) # add attribute field so that we can use collect_training_data function\n",
    "    if gpd_samples.crs is None: # set crs if it somehow is lost\n",
    "        gpd_samples=gpd_samples.set_crs(output_crs)\n",
    "    print('radomly sampled points for class ',i,'\\n',gpd_samples)\n",
    "    \n",
    "    # extract features for the samples\n",
    "    column_names, sampled_data = collect_training_data(gdf=gpd_samples,\n",
    "                                                          dc_query=query,\n",
    "                                                          ncpus=30,\n",
    "#                                                           ncpus=1,\n",
    "                                                          field=class_name, \n",
    "                                                          zonal_stats=zonal_stats,\n",
    "                                                          feature_func=feature_layers,\n",
    "                                                            clean=True,\n",
    "                                                          return_coords=False)\n",
    "    # normalise features before clustering\n",
    "    scaler=scaler.fit(sampled_data[:,1:])\n",
    "    sampled_data=scaler.transform(sampled_data[:,1:])\n",
    "\n",
    "    # fit kmeans model using the sample training data\n",
    "    # first find optimal number of clusters based on Calinski-Harabasz index\n",
    "    highest_score=-999 # initialise score\n",
    "    n_cluster_optimal=2 # initialise number of clusters\n",
    "    kmeans_model_optimal=None # initialise optimal model\n",
    "    labels_optimal=None # initialse optimal clustering results\n",
    "    for n_cluster in range(2,10): # loop to find optimal clusterer\n",
    "        kmeans_model = KMeans(n_clusters=n_cluster, random_state=1).fit(sampled_data)\n",
    "        labels=kmeans_model.predict(sampled_data)\n",
    "        score=metrics.calinski_harabasz_score(sampled_data, labels)\n",
    "        print('Calinski-Harabasz score for ',n_cluster,' clusters is: ',score)\n",
    "        if (highest_score==-999)or(highest_score<score):\n",
    "            highest_score=score\n",
    "            n_cluster_optimal=n_cluster\n",
    "            kmeans_model_optimal=kmeans_model\n",
    "            labels_optimal=labels\n",
    "    print('Best number of clusters for class %s: %s'%(i,n_cluster_optimal))\n",
    "    \n",
    "    # extract features for training points of each class\n",
    "    td_single_class=training_data_unfiltered[training_data_unfiltered[class_name]==i].reset_index(drop=True) # identify training points of the\n",
    "    print('Number of training data collected: ',len(td_single_class))\n",
    "    column_names, model_input = collect_training_data(gdf=td_single_class,\n",
    "                                                      dc_query=query,\n",
    "                                                      ncpus=30, # change here depending on your sandbox instance\n",
    "                                                      field=class_name,\n",
    "                                                      zonal_stats=zonal_stats,\n",
    "                                                      feature_func=feature_layers,\n",
    "                                                      clean=True,\n",
    "                                                      return_coords=True) # extract features of training points\n",
    "    print('Number of training points after removing Nans and Infs: ',model_input.shape[0])\n",
    "\n",
    "    # first covert the training data to pandas\n",
    "    td_single_class_filtered=pd.DataFrame(data=model_input,columns=column_names)\n",
    "\n",
    "    # then to geopandas dataframe\n",
    "    td_single_class_filtered=gpd.GeoDataFrame(td_single_class_filtered, \n",
    "                                    geometry=gpd.points_from_xy(model_input[:,-2], model_input[:,-1],\n",
    "                                                                crs=output_crs))\n",
    "    # normalise features before clustering\n",
    "    model_input=scaler.transform(model_input[:,1:-2])\n",
    "\n",
    "    # predict clustering labels\n",
    "    labels_kmeans = kmeans_model_optimal.predict(model_input)\n",
    "\n",
    "    # append clustering labels to training dataframe\n",
    "    td_single_class_filtered['cluster']=labels_kmeans\n",
    "\n",
    "    # calculate and append cluster frequency of each cluster\n",
    "#     labels_optimal=pd.DataFrame(data=labels_optimal,columns=['cluster']) # calculate cluster frequencies of the random samples\n",
    "#     cluster_frequency=td_single_class_filtered['cluster'].map(labels_optimal['cluster'].value_counts(normalize=True))\n",
    "    cluster_frequency=td_single_class_filtered['cluster'].map(td_single_class_filtered['cluster'].value_counts(normalize=True))\n",
    "    td_single_class_filtered['cluster_frequency']=cluster_frequency\n",
    "\n",
    "    # filter by cluster frequency\n",
    "    td_single_class_filtered=td_single_class_filtered[td_single_class_filtered['cluster_frequency']>=frequency_threshold]\n",
    "    print('Number of training data after filtering: ',len(td_single_class_filtered))\n",
    "\n",
    "    # export filtered training data for this class as geojson file\n",
    "#     td_single_class_filtered.to_file('Results/stratified_random_training_points_scheme_ii_2021_filtered_class_'+str(i)+'.geojson', driver=\"GeoJSON\")\n",
    "    td_single_class_filtered.to_file('Results/manual_random_training_points_scheme_ii_2021_filtered_class_'+str(i)+'.geojson', driver=\"GeoJSON\")\n",
    "    # append the filtered training points of this class to final filtered training data\n",
    "    if td2021_filtered is None:\n",
    "        td2021_filtered=td_single_class_filtered\n",
    "    else:\n",
    "        td2021_filtered=pd.concat([td2021_filtered, td_single_class_filtered])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a94570d9",
   "metadata": {},
   "source": [
    "### export filtered training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16321ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save filtered training data for all classes as geojson file\n",
    "print('filtered training data for 2021:\\n',td2021_filtered)\n",
    "# td2021_filtered.to_file('Results/stratified_random_training_points_scheme_ii_2021_filtered.geojson', driver=\"GeoJSON\")\n",
    "td2021_filtered.to_file('Results/manual_random_training_points_scheme_ii_2021_filtered.geojson', driver=\"GeoJSON\")\n",
    "\n",
    "# save filtered training data as txt file\n",
    "# output_file = \"Results/stratified_random_training_points_scheme_ii_2021_filtered.txt\"\n",
    "output_file = \"Results/manual_random_training_points_scheme_ii_2021_filtered.txt\"\n",
    "td2021_filtered.to_csv(output_file, header=True, index=None, sep=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 05:37:49) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "01a9736267bf300689e8e016092cd01f0c67384dd94651ae6e139a291bc8cc97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
