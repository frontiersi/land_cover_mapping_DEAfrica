{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b95a70b-e46a-405a-9de7-7957efa6170a",
   "metadata": {},
   "source": [
    "This notebook first extracts training data (feature layers) from the Open Data Cube (ODC) of Sentinel-2 multispectral images using the (unfiltered) training datasets in 2021, then filters the extracted training data on a per-class basis using a reference/baseline land cover map based on k-means clustering. The filtered training data will then be used to train a classifier and produce a land cover classification map in 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fccc7e0-768c-4479-a777-6ce9efbf4d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/usr/local/lib/python3.8/dist-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/usr/local/lib/python3.8/dist-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ncpus = 31\n",
      "land cover survey points 2017:\n",
      "       LC_Class_I                        geometry\n",
      "0              1  POINT (761265.704 9740241.901)\n",
      "1              1  POINT (757655.704 9755451.901)\n",
      "2              1  POINT (833365.704 9755131.901)\n",
      "3              1  POINT (768215.704 9703841.901)\n",
      "4              1  POINT (889385.704 9793051.901)\n",
      "...          ...                             ...\n",
      "7095          13  POINT (723725.704 9701531.901)\n",
      "7096          13  POINT (793395.704 9834831.901)\n",
      "7097          13  POINT (762095.704 9811201.901)\n",
      "7098          13  POINT (798315.704 9835261.901)\n",
      "7099          13  POINT (904495.704 9745341.901)\n",
      "\n",
      "[7100 rows x 2 columns]\n",
      "Initial random forest classifcation raster:\n",
      " <xarray.Dataset>\n",
      "Dimensions:      (x: 23234, y: 20992)\n",
      "Coordinates:\n",
      "  * x            (x) float64 7.043e+05 7.044e+05 ... 9.367e+05 9.367e+05\n",
      "  * y            (y) float64 9.887e+06 9.887e+06 ... 9.678e+06 9.678e+06\n",
      "    spatial_ref  int64 0\n",
      "Data variables:\n",
      "    band_data    (y, x) uint8 0 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import datacube\n",
    "import warnings\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from odc.algo import xr_geomedian\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.bandindices import calculate_indices\n",
    "from deafrica_tools.classification import collect_training_data\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from rasterio.enums import Resampling\n",
    "from random_sampling import random_sampling # adapted from function by Chad Burton: https://gist.github.com/cbur24/04760d645aa123a3b1817b07786e7d9f\n",
    "\n",
    "# get number of cpus\n",
    "ncpus=round(get_cpu_quota())\n",
    "print('ncpus = '+str(ncpus))\n",
    "\n",
    "# file paths and attributes\n",
    "traning_points_path = 'Results/manual_number_random_training_points_scheme_ii_2015.geojson'\n",
    "ref_map_path='Results/rwanda_landcover_2015_scheme_ii_classes_merged.tif' # reference land cover map (class merged)\n",
    "class_name = 'LC_Class_I' # class label in integer format\n",
    "output_crs='epsg:32735' # WGS84/UTM Zone 35S\n",
    "\n",
    "# Load unfiltered land cover points and reproject\n",
    "training_data_unfiltered= gpd.read_file(traning_points_path).to_crs(output_crs) # read training points as geopandas dataframe\n",
    "training_data_unfiltered=training_data_unfiltered[[class_name,'geometry']] # select attributes\n",
    "training_data_unfiltered[class_name]=training_data_unfiltered[class_name].astype(int)\n",
    "print('land cover training points (unfiltered):\\n',training_data_unfiltered)\n",
    "\n",
    "# further merge classes if needed\n",
    "# training_data_unfiltered.loc[training_data_unfiltered[class_name]==2,class_name]=1 # Open Forest (2) merged with Dense Forest (1) as Forest (1)\n",
    "# training_data_unfiltered.loc[training_data_unfiltered[class_name]==8,class_name]=6 # Wooded Grassland (8) merged with Open Grassland (6) as Grassland (6)\n",
    "\n",
    "# load reference classification map\n",
    "ref_map_raster = xr.open_dataset(ref_map_path,engine=\"rasterio\").astype(np.uint8).squeeze(\"band\", drop=True)\n",
    "# reproject the raster\n",
    "ref_map_raster= ref_map_raster.rio.reproject(resolution=10, dst_crs=output_crs,resampling=Resampling.nearest)\n",
    "print('Initial random forest classifcation raster:\\n',ref_map_raster) # note: 255 is nodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ff1b5f8-3028-49a2-88cc-a136fc693d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ODC query\n",
    "measurements = ['blue','green','red','red_edge_1','red_edge_2', 'red_edge_3','nir_1','nir_2','swir_1','swir_2']\n",
    "query = {\n",
    "    'time': ('2021-01', '2021-12'),\n",
    "    'measurements': measurements,\n",
    "    'output_crs': output_crs,\n",
    "    'resolution': (-10, 10)\n",
    "}\n",
    "# define a function to feature layers\n",
    "def feature_layers(query): \n",
    "    \n",
    "    #connect to the datacube\n",
    "    dc = datacube.Datacube(app='feature_layers')\n",
    "    ds = load_ard(dc=dc,\n",
    "                  products=['s2_l2a'],\n",
    "                  group_by='solar_day',\n",
    "                  verbose=False,\n",
    "#                   mask_filters=[(\"opening\", 2)], # morphological opening by 2 pixels to remove small masked regions\n",
    "                  **query)\n",
    "    # calculate NDVI\n",
    "    ds = calculate_indices(ds,\n",
    "                           index=['NDVI'],\n",
    "                           drop=False,\n",
    "                           satellite_mission='s2')\n",
    "    \n",
    "    # calculate geomedians within each two-month interval\n",
    "    ds=ds.resample(time='2MS').map(xr_geomedian)\n",
    "    \n",
    "    # interpolate nodata using mean of previous and next observation\n",
    "#     ds=ds.interpolate_na(dim='time',method='linear',use_coordinate=False,fill_value='extrapolate')\n",
    "\n",
    "    # stack multi-temporal measurements and rename them\n",
    "    n_time=ds.dims['time']\n",
    "    list_measurements=list(ds.keys())\n",
    "    ds_stacked=None\n",
    "    for j in range(len(list_measurements)):\n",
    "        for k in range(n_time):\n",
    "            variable_name=list_measurements[j]+'_'+str(k)\n",
    "            # print ('Stacking band ',list_measurements[j],' at time ',k)\n",
    "            measure_single=ds[list_measurements[j]].isel(time=k).rename(variable_name)\n",
    "            if ds_stacked is None:\n",
    "                ds_stacked=measure_single\n",
    "            else:\n",
    "                ds_stacked=xr.merge([ds_stacked,measure_single],compat='override')\n",
    "    return ds_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd905a2-2137-4fb0-8c4b-3098c8c96b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing class  1\n",
      "Class 1.0: sampling at 1000 coordinates\n",
      "radomly sampled points for class  1 \n",
      "                            geometry  LC_Class_I\n",
      "0    POINT (730615.704 9727541.901)           1\n",
      "1    POINT (739245.704 9734471.901)           1\n",
      "2    POINT (757875.704 9789791.901)           1\n",
      "3    POINT (758155.704 9744011.901)           1\n",
      "4    POINT (728235.704 9725391.901)           1\n",
      "..                              ...         ...\n",
      "995  POINT (721355.704 9718181.901)           1\n",
      "996  POINT (755865.704 9730491.901)           1\n",
      "997  POINT (914385.704 9785391.901)           1\n",
      "998  POINT (777185.704 9829621.901)           1\n",
      "999  POINT (759755.704 9754621.901)           1\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "Collecting training data in parallel mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a7796424944065b4911c4db1a52c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n",
      "/usr/local/lib/python3.8/dist-packages/odc/algo/_geomedian.py:106: RuntimeWarning: Mean of empty slice\n",
      "  data = nangeomedian_pcm(xx_data, **kw)\n"
     ]
    }
   ],
   "source": [
    "# parameters and variables for training data filtering\n",
    "lc_classes=training_data_unfiltered[class_name].unique() # get class labels\n",
    "n_samples=1000 # number of samples to train kmeans clusterer\n",
    "zonal_stats = None\n",
    "scaler = StandardScaler() # standard scaler for input data standardisation\n",
    "frequency_threshold=0.1 # threshold of cluter frequency\n",
    "td2021_filtered=None # initialise filtered training data\n",
    "\n",
    "# filtering training data for each class\n",
    "for i in lc_classes:\n",
    "    print('Processing class ',i)\n",
    "    \n",
    "    # generate randomly distributed samples to train and optimise a kmeans clusterer\n",
    "    da_mask=ref_map_raster.band_data # get data array\n",
    "    da_mask=da_mask.where(da_mask==i,np.nan) # replace other class values as nan so they won't be sampled\n",
    "    gpd_samples=random_sampling(da_mask,n_samples,sampling='stratified_random',manual_class_ratios=None,out_fname=None)\n",
    "    gpd_samples=gpd_samples.drop(columns=['spatial_ref','class']) # drop this attribute derived from random_sampling function\n",
    "    gpd_samples[class_name]=int(i) # add attribute field so that we can use collect_training_data function\n",
    "    if gpd_samples.crs is None: # set crs if it somehow is lost\n",
    "        gpd_samples=gpd_samples.set_crs(output_crs)\n",
    "    print('radomly sampled points for class ',i,'\\n',gpd_samples)\n",
    "    \n",
    "    # extract features for the samples\n",
    "    column_names, sampled_data = collect_training_data(gdf=gpd_samples,\n",
    "                                                          dc_query=query,\n",
    "                                                          ncpus=30,\n",
    "#                                                           ncpus=1,\n",
    "                                                          field=class_name, \n",
    "                                                          zonal_stats=zonal_stats,\n",
    "                                                          feature_func=feature_layers,\n",
    "                                                            clean=True,\n",
    "                                                          return_coords=False)\n",
    "    # normalise features before clustering\n",
    "    scaler=scaler.fit(sampled_data[:,1:])\n",
    "    sampled_data=scaler.transform(sampled_data[:,1:])\n",
    "\n",
    "    # fit kmeans model using the sample training data\n",
    "    # first find optimal number of clusters based on Calinski-Harabasz index\n",
    "    highest_score=-999 # initialise score\n",
    "    n_cluster_optimal=2 # initialise number of clusters\n",
    "    kmeans_model_optimal=None # initialise optimal model\n",
    "    labels_optimal=None # initialse optimal clustering results\n",
    "    for n_cluster in range(2,10): # loop to find optimal clusterer\n",
    "        kmeans_model = KMeans(n_clusters=n_cluster, random_state=1).fit(sampled_data)\n",
    "        labels=kmeans_model.predict(sampled_data)\n",
    "        score=metrics.calinski_harabasz_score(sampled_data, labels)\n",
    "        print('Calinski-Harabasz score for ',n_cluster,' clusters is: ',score)\n",
    "        if (highest_score==-999)or(highest_score<score):\n",
    "            highest_score=score\n",
    "            n_cluster_optimal=n_cluster\n",
    "            kmeans_model_optimal=kmeans_model\n",
    "            labels_optimal=labels\n",
    "    print('Best number of clusters for class %s: %s'%(i,n_cluster_optimal))\n",
    "    \n",
    "    # extract features of training points for this class\n",
    "    td_single_class=training_data_unfiltered[training_data_unfiltered[class_name]==i].reset_index(drop=True) # identify training points of the\n",
    "    print('Number of training data collected: ',len(td_single_class))\n",
    "    column_names, model_input = collect_training_data(gdf=td_single_class,\n",
    "                                                      dc_query=query,\n",
    "                                                      ncpus=30,\n",
    "                                                      field=class_name,\n",
    "                                                      zonal_stats=zonal_stats,\n",
    "                                                      feature_func=feature_layers,\n",
    "                                                      clean=True,\n",
    "                                                      return_coords=True) # extract features of training points\n",
    "    print('Number of training points after removing Nans and Infs: ',model_input.shape[0])\n",
    "    # first covert the training data to pandas\n",
    "    td_single_class_filtered=pd.DataFrame(data=model_input,columns=column_names)\n",
    "    # then to geopandas dataframe\n",
    "    td_single_class_filtered=gpd.GeoDataFrame(td_single_class_filtered, \n",
    "                                    geometry=gpd.points_from_xy(model_input[:,-2], model_input[:,-1],\n",
    "                                                                crs=output_crs))\n",
    "    # normalise features before clustering\n",
    "    model_input=scaler.transform(model_input[:,1:-2])\n",
    "    # predict clustering labels\n",
    "    labels_kmeans = kmeans_model_optimal.predict(model_input)\n",
    "    # append clustering labels to training dataframe\n",
    "    td_single_class_filtered['cluster']=labels_kmeans\n",
    "    # calculate and append cluster frequency of each cluster\n",
    "#     labels_optimal=pd.DataFrame(data=labels_optimal,columns=['cluster']) # calculate cluster frequencies of the random samples\n",
    "#     cluster_frequency=td_single_class_filtered['cluster'].map(labels_optimal['cluster'].value_counts(normalize=True))\n",
    "    cluster_frequency=td_single_class_filtered['cluster'].map(td_single_class_filtered['cluster'].value_counts(normalize=True))\n",
    "    td_single_class_filtered['cluster_frequency']=cluster_frequency\n",
    "    # filter by cluster frequency\n",
    "    td_single_class_filtered=td_single_class_filtered[td_single_class_filtered['cluster_frequency']>=frequency_threshold]\n",
    "    print('Number of training data after filtering: ',len(td_single_class_filtered))\n",
    "    # export filtered training data for this class as geojson file\n",
    "#     td_single_class_filtered.to_file('Results/stratified_random_training_points_scheme_ii_2021_filtered_class_'+str(i)+'.geojson', driver=\"GeoJSON\")\n",
    "    td_single_class_filtered.to_file('Results/manual_random_training_points_scheme_ii_2021_filtered_class_'+str(i)+'.geojson', driver=\"GeoJSON\")\n",
    "    # append the filtered training points of this class to final filtered training data\n",
    "    if td2021_filtered is None:\n",
    "        td2021_filtered=td_single_class_filtered\n",
    "    else:\n",
    "        td2021_filtered=pd.concat([td2021_filtered, td_single_class_filtered])\n",
    "        \n",
    "# save filtered training data for all classes as geojson file\n",
    "print('filtered training data for 2021:\\n',td2021_filtered)\n",
    "# td2021_filtered.to_file('Results/stratified_random_training_points_scheme_ii_2021_filtered.geojson', driver=\"GeoJSON\")\n",
    "td2021_filtered.to_file('Results/manual_random_training_points_scheme_ii_2021_filtered.geojson', driver=\"GeoJSON\")\n",
    "# save filtered training data as txt file\n",
    "# output_file = \"Results/stratified_random_training_points_scheme_ii_2021_filtered.txt\"\n",
    "output_file = \"Results/manual_random_training_points_scheme_ii_2021_filtered.txt\"\n",
    "td2021_filtered.to_csv(output_file, header=True, index=None, sep=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
