{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4038fc85",
   "metadata": {},
   "source": [
    "This notebook implements FAO's training data filtering method using k-means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4db0add",
   "metadata": {},
   "source": [
    "### load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50093096",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import datacube\n",
    "import warnings\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from datacube.utils.cog import write_cog\n",
    "from odc.algo import xr_geomedian\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.bandindices import calculate_indices\n",
    "from deafrica_tools.classification import collect_training_data\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from rasterio.enums import Resampling\n",
    "from random_sampling import random_sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3eb312",
   "metadata": {},
   "source": [
    "### parameters and file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27efc1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths and attributes\n",
    "training_signature_path='Results/Mozambique_training_features.txt'\n",
    "\n",
    "rf2017_path='Results/moz_lulc2016_28082019_final_remapped.tif'\n",
    "\n",
    "# tile shapefile to stratify the random sampling to fit into memory\n",
    "tiles_shp='Data/Mozambique_tiles_biggest1.shp'\n",
    "\n",
    "output_crs='epsg:32736' # WGS84/UTM Zone 36S\n",
    "measurements = ['blue','green','red','red_edge_1','red_edge_2', 'red_edge_3','nir_1','nir_2','swir_1','swir_2','NDVI']\n",
    "class_name = 'LC_Class_I' # class label in integer format\n",
    "column_names=[class_name]\n",
    "for measurement in measurements:\n",
    "    for i in range(1,12,2):\n",
    "        column_names.append(measurement+'_'+str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822c2af9",
   "metadata": {},
   "source": [
    "### load training data, tiles and reference map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e3b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data2017= pd.read_csv(training_signature_path,sep=' ')\n",
    "training_data2017=training_data2017[column_names] # select attributes\n",
    "training_data2017[class_name]=training_data2017[class_name].astype(int)\n",
    "print('land cover survey points 2017:\\n',training_data2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25090c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles=gpd.read_file(tiles_shp).to_crs(output_crs)\n",
    "# get bounding boxes of tiles\n",
    "tile_bboxes=tiles.bounds\n",
    "print('tile boundaries for Mozambique: \\n',tile_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d8c30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_2017_raster = xr.open_dataset(rf2017_path,engine=\"rasterio\").astype(np.uint8).squeeze(\"band\", drop=True)\n",
    "# # reproject the raster\n",
    "rf_2017_raster= rf_2017_raster.rio.reproject(resolution=30, dst_crs=output_crs,resampling=Resampling.nearest)\n",
    "rf_2017_raster=rf_2017_raster.band_data\n",
    "print('Reference land cover classifcation raster:\\n',rf_2017_raster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a0fd62",
   "metadata": {},
   "source": [
    "### define queries and feature layer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff1b5f8-3028-49a2-88cc-a136fc693d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm_s2_available=False\n",
    "\n",
    "measurements = ['blue','green','red','red_edge_1','red_edge_2', 'red_edge_3','nir_1','nir_2','swir_1','swir_2']\n",
    "resolution = (-10,10)\n",
    "\n",
    "if gm_s2_available:\n",
    "    query = {\n",
    "        'time': ('2021-01', '2021-12'),\n",
    "        'measurements': measurements,\n",
    "        'output_crs': output_crs,\n",
    "        'resolution': resolution\n",
    "    }\n",
    "    # define a function to feature layers\n",
    "    def feature_layers(query): \n",
    "        # connect to the datacube so we can access DE Africa data\n",
    "        dc = datacube.Datacube(app='rolling geomedians')\n",
    "        # load rolling geomedians\n",
    "        ds = dc.load(product='gm_s2_rolling',measurements=measurements,\n",
    "                     group_by='solar_day',**query)\n",
    "        ds = calculate_indices(ds,\n",
    "                           index=['NDVI'],\n",
    "                           drop=False,\n",
    "                           satellite_mission='s2')\n",
    "        n_time=ds.dims['time'] # 12\n",
    "        list_measurements=list(ds.keys())\n",
    "        ds_stacked=None\n",
    "        for j in range(len(list_measurements)):\n",
    "            for k in range(1,n_time,2): # extract the six months 2021-01, 2021-03, 2021-05,... 2021-11\n",
    "                variable_name=list_measurements[j]+'_'+str(k)\n",
    "                measure_single=ds[list_measurements[j]].isel(time=k).rename(variable_name)\n",
    "                if ds_stacked is None:\n",
    "                    ds_stacked=measure_single\n",
    "                else:\n",
    "                    ds_stacked=xr.merge([ds_stacked,measure_single],compat='override')\n",
    "        return ds_stacked\n",
    "else:\n",
    "    query = {\n",
    "        'time': ('2020-12', '2021-12'),\n",
    "        'measurements': measurements,\n",
    "        'output_crs': output_crs,\n",
    "        'resolution': resolution\n",
    "    }\n",
    "    # define a function to feature layers\n",
    "    def feature_layers(query):\n",
    "        dc = datacube.Datacube(app='rolling geomedians')\n",
    "        # load Sentinel-2 analysis ready data\n",
    "        ds = load_ard(dc=dc,\n",
    "                      products=['s2_l2a'],\n",
    "                      group_by='solar_day',\n",
    "                      verbose=False,\n",
    "                      **query)\n",
    "        ds = calculate_indices(ds,\n",
    "                               index=['NDVI'],\n",
    "                               drop=False,\n",
    "                               satellite_mission='s2')\n",
    "        # calculate rolling geomedians\n",
    "        time_slices=[('2020-12','2021-02'),('2021-02','2021-04'),('2021-04','2021-06'),\n",
    "                     ('2021-06','2021-08'),('2021-08','2021-10'),('2021-10','2021-12')]\n",
    "        ds_rolling=None\n",
    "        for i in range(len(time_slices)):\n",
    "            ds_single=xr_geomedian(ds.sel(time=slice(time_slices[i][0],time_slices[i][1]))).assign_coords({'time':time_slices[i][0]})\n",
    "            if ds_rolling is None:\n",
    "                ds_rolling=ds_single\n",
    "            else:\n",
    "                ds_rolling=xr.concat([ds_rolling,ds_single],dim='time')\n",
    "        # stackmulti-temporal measurements and rename them\n",
    "        n_time=ds_rolling.dims['time']\n",
    "        list_measurements=list(ds_rolling.keys())\n",
    "        list_stack_measures=[]\n",
    "        for j in range(len(list_measurements)):\n",
    "            for k in range(n_time):\n",
    "#                 variable_name=list_measurements[j]+'_'+str(k)\n",
    "                variable_name=list_measurements[j]+'_'+str(2*k+1) # to keep consistent with above case\n",
    "                measure_single=ds_rolling[list_measurements[j]].isel(time=k).rename(variable_name)\n",
    "                list_stack_measures.append(measure_single)\n",
    "        ds_stacked=xr.merge(list_stack_measures,compat='override')\n",
    "        return ds_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1144b1a",
   "metadata": {},
   "source": [
    "### Generate random samples - stratified by class and generated per tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc906255",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_classes=training_data2017[class_name].unique() # get class labels\n",
    "n_class=len(lc_classes)\n",
    "print('land cover classes:\\n',lc_classes)\n",
    "n_tile=len(tile_bboxes)\n",
    "n_samples=1000 # number of random samples per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7348b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_gpd_samples='Results/stratified_random_samples.geojson'\n",
    "if os.path.exists(path_gpd_samples):\n",
    "    gpd_samples=gpd.read_file(path_gpd_samples)\n",
    "else:\n",
    "    # calculate number of pixels belong to each class for each tile\n",
    "    n_class_tiles=np.zeros((n_class,n_tile))\n",
    "    for j in range(n_tile):\n",
    "        print('calculating stasts for tile ',j)\n",
    "        tile_raster=rf_2017_raster.rio.clip([tiles.iloc[j].geometry],crs=output_crs,drop=True,from_disk=True)\n",
    "        for i in range(n_class):\n",
    "            n_class_tiles[i,j]=np.sum(tile_raster.to_numpy()==lc_classes[i])\n",
    "    n_class_total=np.sum(n_class_tiles,axis=1) # total number of pixels belong to each class\n",
    "    del tile_raster\n",
    "\n",
    "    # extract random samples from each tile\n",
    "    gpd_samples=None\n",
    "    for j in range(n_tile):\n",
    "        print('Extracting random samples for tile ',j)\n",
    "        tile_raster = rf_2017_raster.rio.clip([tiles.iloc[j].geometry],crs=output_crs,drop=True,from_disk=True)\n",
    "        n_samples_tile=[int((n_samples*n_class_tiles[i,j])/n_class_total[i]) for i in range(n_class)] # number of samples per class\n",
    "        n_samples_tile_valid={str(lc_classes[i]):n_samples_tile[i] for i in range(n_class) if n_samples_tile[i]>0} # remove class absent on this tile\n",
    "        gpd_samples_tile=random_sampling(tile_raster,n_samples_tile,sampling='manual',\n",
    "                                         manual_class_ratios=n_samples_tile_valid,\n",
    "                                         out_fname=None)\n",
    "        if gpd_samples is None:\n",
    "            gpd_samples=gpd_samples_tile\n",
    "        else:\n",
    "            gpd_samples=pd.concat([gpd_samples,gpd_samples_tile])\n",
    "    del tile_raster\n",
    "\n",
    "    gpd_samples[class_name]=gpd_samples['class'].astype(int) # add class attribute field\n",
    "    gpd_samples=gpd_samples.reset_index(drop=True).drop(columns=['spatial_ref','class']) # drop attribute derived from random_sampling function\n",
    "    if gpd_samples.crs is None:\n",
    "        gpd_samples=gpd_samples.set_crs(output_crs)\n",
    "    # export to disk\n",
    "    gpd_samples.to_file(path_gpd_samples, driver=\"GeoJSON\")\n",
    "gpd_samples=gpd_samples[[class_name,'geometry']]\n",
    "print('radomly sampled points for all classes: \\n',gpd_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa6b50c",
   "metadata": {},
   "source": [
    "### Extract features for the random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59a8332",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = 'Results/stratified_random_samples_features.txt'\n",
    "if os.path.exists(output_file):\n",
    "    rand_samples_features=pd.read_csv(output_file,sep=' ')\n",
    "else:\n",
    "    ncpus=round(get_cpu_quota())\n",
    "    print('ncpus = '+str(ncpus))\n",
    "    # extract data for the random samples\n",
    "    column_names, model_input = collect_training_data(gdf=gpd_samples,\n",
    "                                                        dc_query=query,\n",
    "                                                        ncpus=ncpus,\n",
    "                                                        field=class_name, \n",
    "                                                        zonal_stats=None,\n",
    "                                                        feature_func=feature_layers,\n",
    "                                                        return_coords=False)\n",
    "    rand_samples_features=pd.DataFrame(data=model_input,columns=column_names)\n",
    "    #Export files to disk\n",
    "    rand_samples_features.to_csv(output_file, header=True, index=None, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de6812c",
   "metadata": {},
   "source": [
    "### kmeans clustering and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb7811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clusters_KMeans(data,min_cluster=5,max_cluster=20):\n",
    "    highest_score=-999\n",
    "    n_cluster_optimal=min_cluster\n",
    "    kmeans_model_optimal=None # initialise optimal model parameters\n",
    "    labels_optimal=None\n",
    "    if min_cluster==max_cluster:\n",
    "        print('Implementing kmeans clustering with number of clusters: ',max_cluster)\n",
    "        kmeans_model_optimal = KMeans(n_clusters=max_cluster, random_state=1).fit(data)\n",
    "        labels_optimal=kmeans_model_optimal.predict(data)\n",
    "        n_cluster_optimal=max_cluster\n",
    "    else:\n",
    "        for n_cluster in range(min_cluster,max_cluster+1):\n",
    "            kmeans_model = KMeans(n_clusters=n_cluster, random_state=1).fit(data)\n",
    "            labels=kmeans_model.predict(data)\n",
    "            score=metrics.calinski_harabasz_score(data, labels)\n",
    "            print('Calinski-Harabasz score for ',n_cluster,' clusters is: ',score)\n",
    "            if (highest_score==-999)or(highest_score<score):\n",
    "                highest_score=score\n",
    "                n_cluster_optimal=n_cluster\n",
    "                kmeans_model_optimal=kmeans_model\n",
    "                labels_optimal=labels\n",
    "        print('Best number of clusters: %s'%(n_cluster_optimal))\n",
    "    return n_cluster_optimal,kmeans_model_optimal,labels_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26d29e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() # standard scaler for input data standardisation\n",
    "frequency_threshold=0.1 # threshold of cluter frequency\n",
    "optimal_clusters={12:None, 61:None, 41:None, 72:None, 74:None, 31:None, 51:None, 44:None,\n",
    "                  75:None, 21:None, 71:None, 70:None, 11:None}\n",
    "training_features_filtered=None \n",
    "for class_value in lc_classes:\n",
    "    print('Processing class ',class_value)\n",
    "    rand_features_single_class=rand_samples_features[rand_samples_features[class_name]==class_value].reset_index(drop=True)\n",
    "    np_rand_features=rand_features_single_class.to_numpy()[:,1:]\n",
    "    # standardise features\n",
    "    scaler.fit(np_rand_features)\n",
    "    np_rand_features=scaler.transform(np_rand_features)\n",
    "    \n",
    "    if optimal_clusters[class_value] is None:\n",
    "        n_cluster_optimal,kmeans_model_optimal,labels_optimal=find_clusters_KMeans(np_rand_features,min_cluster=5,max_cluster=20)\n",
    "    else:\n",
    "        n_cluster_optimal,kmeans_model_optimal,labels_optimal=find_clusters_KMeans(np_rand_features,min_cluster=optimal_clusters[class_value],max_cluster=optimal_clusters[class_value])\n",
    "\n",
    "        # subset original training points for this class\n",
    "    td_single_class=training_data2017[training_data2017[class_name]==class_value].reset_index(drop=True)\n",
    "    print('Number of training pints for the class: ',len(td_single_class))\n",
    "    np_td_single_class=td_single_class.to_numpy()[:,1:]\n",
    "    # standardise features\n",
    "    np_td_single_class=scaler.transform(np_td_single_class)\n",
    "    # predict clustering labels\n",
    "    labels_kmeans = kmeans_model_optimal.predict(np_td_single_class)\n",
    "    # append clustering results to pixel coordinates\n",
    "    td_single_class['cluster']=labels_kmeans\n",
    "\n",
    "    cluster_frequency=td_single_class['cluster'].map(td_single_class['cluster'].value_counts(normalize=True)) # calculate cluster frequencies for the training samples\n",
    "    td_single_class['cluster_frequency']=cluster_frequency # append as a column\n",
    "    td_single_class_filtered=td_single_class[td_single_class['cluster_frequency']>=frequency_threshold] # filter by cluster frequency\n",
    "    print('Number of training data after filtering: ',len(td_single_class_filtered))\n",
    "    \n",
    "    # append the filtered training points of this class to final filtered training data\n",
    "    if training_features_filtered is None:\n",
    "        training_features_filtered=td_single_class_filtered\n",
    "    else:\n",
    "        training_features_filtered=pd.concat([training_features_filtered, td_single_class_filtered])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7e5f36",
   "metadata": {},
   "source": [
    "### export filtered training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a837a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('filtered training data for 2021:\\n',training_features_filtered)\n",
    "output_file = \"Results/Mozambique_training_features_filtered.txt\"\n",
    "training_features_filtered.to_csv(output_file, header=True, index=None, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f456cb-13a2-4576-8a15-54ebcbc0a0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "01a9736267bf300689e8e016092cd01f0c67384dd94651ae6e139a291bc8cc97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
