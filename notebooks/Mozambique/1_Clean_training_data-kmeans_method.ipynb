{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4038fc85",
   "metadata": {},
   "source": [
    "This notebook implements FAO's training data filtering method using k-means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4db0add",
   "metadata": {},
   "source": [
    "### load packages and get number of cpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50093096",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import datacube\n",
    "import warnings\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from odc.algo import xr_geomedian\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.bandindices import calculate_indices\n",
    "from deafrica_tools.classification import collect_training_data\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from rasterio.enums import Resampling\n",
    "from random_sampling import random_sampling # adapted from function by Chad Burton: https://gist.github.com/cbur24/04760d645aa123a3b1817b07786e7d9f\n",
    "\n",
    "ncpus=round(get_cpu_quota())\n",
    "print('ncpus = '+str(ncpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3eb312",
   "metadata": {},
   "source": [
    "### load input file paths and set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fccc7e0-768c-4479-a777-6ce9efbf4d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths and attributes\n",
    "# traning_points_path = 'Data/trainning_samples_FNDS_II_SOM_2016.geojson'\n",
    "traning_points_path = 'Results/stratified_random_training_points_lulc_2016_balanced.geojson'\n",
    "# rf2017_path='Data/Landcover_map_ODC_Brazil_2015_2016.tif'\n",
    "rf2017_path='Data/moz_lulc2016_28082019_final.tif'\n",
    "tiles_shp='Data/Mozambique_tiles_biggest1.shp'\n",
    "class_name = 'LC_Class_I' # class label in integer format\n",
    "crs='epsg:32736' # WGS84/UTM Zone 36S\n",
    "\n",
    "# Load reference land cover survey points and reproject\n",
    "training_data2017= gpd.read_file(traning_points_path).to_crs(crs) # read training points as geopandas dataframe\n",
    "training_data2017=training_data2017[[class_name,'geometry']] # select attributes\n",
    "print('land cover survey points 2017:\\n',training_data2017)\n",
    "\n",
    "# get bounding boxes of tiles\n",
    "tiles=gpd.read_file(tiles_shp).to_crs(crs)\n",
    "tile_bboxes=tiles.bounds\n",
    "print('tile boundaries for Mozambique: \\n',tile_bboxes)\n",
    "\n",
    "# load initial classification map\n",
    "rf_2017_raster = xr.open_dataset(rf2017_path,engine=\"rasterio\").astype(np.uint8).squeeze(\"band\", drop=True)\n",
    "# # reproject the raster\n",
    "# rf_2017_raster= rf_2017_raster.rio.reproject(resolution=10, dst_crs=crs,resampling=Resampling.nearest)\n",
    "rf_2017_raster=rf_2017_raster.band_data\n",
    "print('Reference land cover classifcation raster:\\n',rf_2017_raster) # note: 255 is nodata\n",
    "# get class labels\n",
    "lc_classes=training_data2017[class_name].unique() \n",
    "print('land cover classes:\\n',lc_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a0fd62",
   "metadata": {},
   "source": [
    "### define queries and feature layer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff1b5f8-3028-49a2-88cc-a136fc693d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill_nan_value=-999 # value to replace nans in query results\n",
    "measurements = ['blue','green','red','red_edge_1','red_edge_2', 'red_edge_3','nir_1','nir_2','swir_1','swir_2']\n",
    "query = {\n",
    "    'time': ('2021-01', '2021-12'),\n",
    "    'measurements': measurements,\n",
    "    'output_crs': crs,\n",
    "    'resolution': (-10, 10)\n",
    "}\n",
    "# define a function to feature layers\n",
    "def feature_layers(query): \n",
    "    #connect to the datacube\n",
    "    dc = datacube.Datacube(app='feature_layers')\n",
    "    ds = load_ard(dc=dc,\n",
    "                  products=['s2_l2a'],\n",
    "                  group_by='solar_day',\n",
    "                  verbose=False,\n",
    "#                   mask_filters=[(\"opening\", 2)], # morphological opening by 2 pixels to remove small masked regions\n",
    "                  **query)\n",
    "    ds = calculate_indices(ds,\n",
    "                           index=['NDVI'],\n",
    "                           drop=False,\n",
    "                           satellite_mission='s2')\n",
    "    # interpolate nodata using mean of previous and next observation\n",
    "#     ds=ds.interpolate_na(dim='time',method='linear',use_coordinate=False,fill_value='extrapolate')\n",
    "#     ds=ds.interpolate_na(dim='time',method='linear',use_coordinate=False)\n",
    "    # calculate geomedians within each two-month interval\n",
    "    ds=ds.resample(time='2MS').map(xr_geomedian)\n",
    "    # replace nan with a value so that the collect_training_data function will work\n",
    "#     ds=ds.fillna(fill_nan_value)\n",
    "    # stack multi-temporal measurements and rename them\n",
    "    n_time=ds.dims['time']\n",
    "    list_measurements=list(ds.keys())\n",
    "    ds_stacked=None\n",
    "    for j in range(len(list_measurements)):\n",
    "        for k in range(n_time):\n",
    "            variable_name=list_measurements[j]+'_'+str(k)\n",
    "            # print ('Stacking band ',list_measurements[j],' at time ',k)\n",
    "            measure_single=ds[list_measurements[j]].isel(time=k).rename(variable_name)\n",
    "            if ds_stacked is None:\n",
    "                ds_stacked=measure_single\n",
    "            else:\n",
    "                ds_stacked=xr.merge([ds_stacked,measure_single],compat='override')\n",
    "    return ds_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c80b19c",
   "metadata": {},
   "source": [
    "### per-class training data filter using k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd905a2-2137-4fb0-8c4b-3098c8c96b29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_samples=1000 # number of random samples to optimise number of clusters for kmeans\n",
    "zonal_stats = None\n",
    "scaler = StandardScaler() # standard scaler for input data standardisation\n",
    "frequency_threshold=0.05 # threshold of cluter frequency\n",
    "td2021_filtered=None # filtered training data\n",
    "# filtering training data for each class\n",
    "for i in lc_classes:\n",
    "    #i=1 # test for first class\n",
    "    print('Processing class ',i)\n",
    "    gpd_samples=None\n",
    "    n_total=np.sum(rf_2017_raster.to_numpy()==i)\n",
    "    # generate randomly sampled data to fit and optimise a kmeans clusterer\n",
    "    for n in range(len(tile_bboxes)):\n",
    "        print('stratified random sampling from tile ',n)\n",
    "        da_mask=rf_2017_raster.rio.clip([tiles.iloc[n].geometry],crs=crs,drop=True)\n",
    "        da_mask=da_mask.rio.reproject(dst_crs=crs,resampling=Resampling.nearest)\n",
    "        n_samples_tile=n_samples*np.sum(da_mask.to_numpy()==i)/n_total\n",
    "        if n_samples_tile>0:\n",
    "            gpd_samples_tile=random_sampling(da_mask,n_samples_tile,sampling='manual',\n",
    "                                             manual_class_ratios={str(i):n_samples_tile},out_fname=None)\n",
    "            if gpd_samples is None:\n",
    "                gpd_samples=gpd_samples_tile\n",
    "            else:\n",
    "                gpd_samples=pd.concat([gpd_samples,gpd_samples_tile])\n",
    "    # get data array\n",
    "#     da_mask=da_mask.where(da_mask==i,np.nan) # replace other class values as nan so they won't be sampled (comment due to large memory required)\n",
    "#     gpd_samples=random_sampling(da_mask,n_samples,sampling='stratified_random',manual_class_ratios=None,out_fname=None)\n",
    "#     gpd_samples=random_sampling(da_mask,n_samples,sampling='manual',manual_class_ratios={str(i):n_samples},out_fname=None)\n",
    "    gpd_samples=gpd_samples.reset_index(drop=True).drop(columns=['spatial_ref','class']) # drop this attribute derived from random_sampling function\n",
    "    gpd_samples[class_name]=i # add attribute field so that we can use collect_training_data function\n",
    "    if gpd_samples.crs is None:\n",
    "        gpd_samples=gpd_samples.set_crs(crs)\n",
    "    print('radomly sampled points for class ',i,'\\n',gpd_samples)\n",
    "    # extract data for the random samples\n",
    "    column_names, sampled_data = collect_training_data(gdf=gpd_samples,\n",
    "                                                          dc_query=query,\n",
    "                                                          ncpus=ncpus,\n",
    "#                                                           ncpus=1,\n",
    "                                                          field=class_name, \n",
    "                                                          zonal_stats=zonal_stats,\n",
    "                                                          feature_func=feature_layers,\n",
    "                                                          return_coords=False)\n",
    "    # standardise features\n",
    "    scaler=scaler.fit(sampled_data[:,1:])\n",
    "    sampled_data=scaler.transform(sampled_data[:,1:])\n",
    "    # fit kmeans model using the sample training data\n",
    "    # first find optimal number of clusters based on Calinski-Harabasz index\n",
    "    highest_score=-999\n",
    "    n_cluster_optimal=2\n",
    "    kmeans_model_optimal=None # initialise optimal model parameters\n",
    "    labels_optimal=None\n",
    "    for n_cluster in range(2,10):\n",
    "        kmeans_model = KMeans(n_clusters=n_cluster, random_state=1).fit(sampled_data)\n",
    "        labels=kmeans_model.predict(sampled_data)\n",
    "        score=metrics.calinski_harabasz_score(sampled_data, labels)\n",
    "        print('Calinski-Harabasz score for ',n_cluster,' clusters is: ',score)\n",
    "        if (highest_score==-999)or(highest_score<score):\n",
    "            highest_score=score\n",
    "            n_cluster_optimal=n_cluster\n",
    "            kmeans_model_optimal=kmeans_model\n",
    "            labels_optimal=labels\n",
    "    print('Best number of clusters for class %s: %s'%(i,n_cluster_optimal))\n",
    "    \n",
    "    # subset original training points for this class\n",
    "    td_single_class=training_data2017[training_data2017[class_name]==i].reset_index(drop=True)\n",
    "    print('Number of training data collected: ',len(td_single_class))\n",
    "    column_names, model_input = collect_training_data(gdf=td_single_class,\n",
    "                                                      dc_query=query,\n",
    "                                                      ncpus=ncpus,\n",
    "                                                      field=class_name,\n",
    "                                                      zonal_stats=zonal_stats,\n",
    "                                                      feature_func=feature_layers,\n",
    "                                                      clean=True,\n",
    "                                                      return_coords=True)\n",
    "    print('Number of training data after removing Nans and Infs: ',model_input.shape[0])\n",
    "    # first covert the training data to pandas\n",
    "    td_single_class_filtered=pd.DataFrame(data=model_input,columns=column_names)\n",
    "    # then to geopandas dataframe\n",
    "    td_single_class_filtered=gpd.GeoDataFrame(td_single_class_filtered, \n",
    "                                    geometry=gpd.points_from_xy(model_input[:,-2], model_input[:,-1],\n",
    "                                                                crs=crs))\n",
    "    # normalisation before clustering\n",
    "    model_input=scaler.transform(model_input[:,1:-2])\n",
    "    # predict clustering labels\n",
    "    labels_kmeans = kmeans_model_optimal.predict(model_input)\n",
    "    # append clustering results to pixel coordinates\n",
    "    td_single_class_filtered['cluster']=labels_kmeans\n",
    "    # append frequency of each cluster\n",
    "#     labels_optimal=pd.DataFrame(data=labels_optimal,columns=['cluster']) # calculate cluster frequencies of the random samples\n",
    "#     cluster_frequency=td_single_class_filtered['cluster'].map(labels_optimal['cluster'].value_counts(normalize=True))\n",
    "    cluster_frequency=td_single_class_filtered['cluster'].map(td_single_class_filtered['cluster'].value_counts(normalize=True))\n",
    "    td_single_class_filtered['cluster_frequency']=cluster_frequency\n",
    "#     print('filtered training data: \\n',td_single_class_filtered[td_single_class_filtered['cluster_frequency']<frequency_threshold])\n",
    "    # filter by cluster frequency\n",
    "    td_single_class_filtered=td_single_class_filtered[td_single_class_filtered['cluster_frequency']>=frequency_threshold]\n",
    "    print('Number of training data after filtering: ',len(td_single_class_filtered))\n",
    "    # export filtered training data for this class as shapefile (will encounter 10-character limit for attributes)\n",
    "#     td_single_class_filtered.to_file('Results/landcover_td2021_filtered_DEAfrica_new_class_'+str(i)+'.shp')\n",
    "    # export filtered training data for this class as geojson file\n",
    "#     td_single_class_filtered.to_file('Results/landcover_td2021_filtered_class_'+str(i)+'.geojson', driver=\"GeoJSON\")\n",
    "    td_single_class_filtered.to_file('Results/stratified_random_training_points_lulc_2016_balanced_2021_filtered_class_'+str(i)+'.geojson', driver=\"GeoJSON\")\n",
    "    # append the filtered training points of this class to final filtered training data\n",
    "    if td2021_filtered is None:\n",
    "        td2021_filtered=td_single_class_filtered\n",
    "    else:\n",
    "        td2021_filtered=pd.concat([td2021_filtered, td_single_class_filtered])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7e5f36",
   "metadata": {},
   "source": [
    "### export filtered training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a837a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training data for all classes\n",
    "print('filtered training data for 2021:\\n',td2021_filtered)\n",
    "# td2021_filtered.to_file('Results/landcover_td2021_filtered.geojson', driver=\"GeoJSON\")\n",
    "td2021_filtered.to_file('Results/stratified_random_training_points_lulc_2016_balanced_2021_filtered.geojson', driver=\"GeoJSON\")\n",
    "\n",
    "# export the filtered training data as txt file\n",
    "# output_file = \"Results/landcover_td2021_filtered.txt\"\n",
    "output_file = \"Results/stratified_random_training_points_lulc_2016_balanced_2021_filtered.txt\"\n",
    "td2021_filtered.to_csv(output_file, header=True, index=None, sep=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('geoenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 05:37:49) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "01a9736267bf300689e8e016092cd01f0c67384dd94651ae6e139a291bc8cc97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
