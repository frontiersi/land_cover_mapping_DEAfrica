{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook extracts training features from the Open Data Cube (ODC) of Sentinel-2 multispectral images, using unfiltered) training data in a previous year. The features include bi-monthly geomedian of Sentinel-2 bands and semi-annual Median Absolute Deviations (MADs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load packages and get number of cpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import datacube\n",
    "import warnings\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from odc.algo import xr_geomedian\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.bandindices import calculate_indices\n",
    "from deafrica_tools.classification import collect_training_data\n",
    "\n",
    "ncpus=round(get_cpu_quota())\n",
    "print('ncpus = '+str(ncpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input files and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths and attributes\n",
    "traning_points_path = 'Data/train_poly_848_20171124.shp' # please replace with your own training data \n",
    "class_name = 'Class_I' # class label in integer format\n",
    "crs='epsg:32736' # WGS84/UTM Zone 36S\n",
    "zonal_stats = None\n",
    "\n",
    "training_points_2017= gpd.read_file(traning_points_path).to_crs(crs) # read training points as geopandas dataframe\n",
    "training_points_2017=training_points_2017[[class_name,'geometry']] # select attributes\n",
    "training_points_2017[class_name]=training_points_2017[class_name].astype(int)\n",
    "print('Training points in 2016:\\n',training_points_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define query and feature layer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define query\n",
    "query = {\n",
    "    'time': ('2021-01', '2021-12'),\n",
    "    'output_crs': crs,\n",
    "    'resolution': (-10, 10)\n",
    "}\n",
    "# define a function to feature layers\n",
    "def feature_layers(query):\n",
    "    measurements = ['blue','green','red','red_edge_1','red_edge_2', 'red_edge_3','nir_1','swir_1','swir_2']\n",
    "    measurements_MAD=['smad','emad','bcmad']\n",
    "    #connect to the datacube\n",
    "    dc = datacube.Datacube(app='feature_layers')\n",
    "    ds = load_ard(dc=dc,\n",
    "                  products=['s2_l2a'],\n",
    "                  measurements=measurements,\n",
    "                  group_by='solar_day',\n",
    "                  verbose=False,\n",
    "#                   mask_filters=[(\"opening\", 2)], # morphological opening by 2 pixels to remove small masked regions\n",
    "                  **query)\n",
    "    ds = calculate_indices(ds,\n",
    "                           index=['NDVI'],\n",
    "                           drop=False,\n",
    "                           satellite_mission='s2')\n",
    "    # interpolate nodata using mean of previous and next observation\n",
    "#     ds=ds.interpolate_na(dim='time',method='linear',use_coordinate=False,fill_value='extrapolate')\n",
    "    # calculate geomedians within each two-month interval\n",
    "    ds=ds.resample(time='2MS').map(xr_geomedian)\n",
    "    # stack multi-temporal measurements and rename them\n",
    "    n_time=ds.dims['time']\n",
    "    list_measurements=list(ds.keys())\n",
    "    ds_stacked=None\n",
    "    for j in range(len(list_measurements)):\n",
    "        for k in range(n_time):\n",
    "            variable_name=list_measurements[j]+'_'+str(k)\n",
    "            # print ('Stacking band ',list_measurements[j],' at time ',k)\n",
    "            measure_single=ds[list_measurements[j]].isel(time=k).rename(variable_name)\n",
    "            if ds_stacked is None:\n",
    "                ds_stacked=measure_single\n",
    "            else:\n",
    "                ds_stacked=xr.merge([ds_stacked,measure_single],compat='override')\n",
    "    # load semiannual MADs\n",
    "    ds_mads=dc.load(product='gm_s2_semiannual',\n",
    "                    measurements=measurements_MAD,\n",
    "                    **query\n",
    "                   )\n",
    "    # stack multi-temporal bands as variables\n",
    "    n_time=ds_mads.dims['time']\n",
    "    list_measurements=list(ds_mads.keys())\n",
    "    for j in range(len(list_measurements)):\n",
    "        for k in range(n_time):\n",
    "            variable_name=list_measurements[j]+'_'+str(k)\n",
    "            measure_single=ds_mads[list_measurements[j]].isel(time=k).rename(variable_name)\n",
    "            ds_stacked=xr.merge([ds_stacked,measure_single],compat='override')\n",
    "    return ds_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract training features\n",
    "column_names, model_input = collect_training_data(gdf=training_points_2017,\n",
    "                                                  dc_query=query,\n",
    "#                                                   ncpus=1,\n",
    "                                                  ncpus=35,\n",
    "                                                  field=class_name,\n",
    "                                                  zonal_stats=zonal_stats,\n",
    "                                                  feature_func=feature_layers,\n",
    "                                                  return_coords=True)\n",
    "print('Number of training data after removing Nans and Infs: ',model_input.shape[0])\n",
    "# first covert the training data to pandas\n",
    "training_data_2017=pd.DataFrame(data=model_input,columns=column_names)\n",
    "# then to geopandas dataframe\n",
    "training_data_2017=gpd.GeoDataFrame(training_data_2017, \n",
    "                                geometry=gpd.points_from_xy(model_input[:,-2], model_input[:,-1],\n",
    "                                                            crs=crs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### export training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file='Results/train_poly_848_20171124_signatures_2021.geojson'\n",
    "training_data_2017.to_file(output_file, driver=\"GeoJSON\")\n",
    "# also save as txt file\n",
    "output_file ='Results/train_poly_848_20171124_signatures_2021.txt'\n",
    "training_data_2017.to_csv(output_file, header=True, index=None, sep=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('geoenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01a9736267bf300689e8e016092cd01f0c67384dd94651ae6e139a291bc8cc97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
