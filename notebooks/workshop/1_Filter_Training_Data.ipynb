{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4555d86-4eb4-4185-93d8-52f55da43df9",
   "metadata": {},
   "source": [
    "# Filter Training Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f767b5b-dd6e-48f8-aba4-259d72d8804a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Background\n",
    "\n",
    "It is not uncommon that existing training data were collected at different time period than the study period. This means the dataset may not reflect the real ground cover due to temporal changes. FAO adopted a training data filtering method for any given reference year that is within a time span (e.g. 5 years) from an existing baseline, and tested the method in the production of land cover mapping for Lesotho. It is assumed that the majority of reference labels will remain valid from one year to the previous/next. Based on this assumption, the reference labels which have changed are the minority, and should be detectable through the use of outlier detection methods like K-Means clustering. More details on the method and how it works for Lesotho can be found in the published paper ([De Simone et al 2022](https://www.mdpi.com/2072-4292/14/14/3294))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ec80f-8f94-45f9-808c-712a572f8d89",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "This notebook will implement FAO's automatic filtering of a training dataset for a target year using points from a geojson or shapefile and a reference classification map of a previous year. The steps include:\n",
    "1. Load extracted training features\n",
    "2. Generate stratified random samples for each class and extract their features using `random_sampling` and `collect_training_data`\n",
    "3. Train K-Means models using the features of the random samples\n",
    "4. Apply clustering on training features and remove minor clusters, i.e. cluster size smaller than 5% of overall sample size\n",
    "5. Export the filtered training data to disk for use in subsequent scripts\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdba3177-6db9-4440-9c8b-d5437f41e7f4",
   "metadata": {},
   "source": [
    "### Load packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fccc7e0-768c-4479-a777-6ce9efbf4d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import datacube\n",
    "import warnings\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from odc.algo import xr_geomedian\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.bandindices import calculate_indices\n",
    "from deafrica_tools.classification import collect_training_data\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from rasterio.enums import Resampling\n",
    "from random_sampling import random_sampling # adapted from function by Chad Burton: https://gist.github.com/cbur24/04760d645aa123a3b1817b07786e7d9f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efee6d6-c909-427c-bcb7-2943ae33b6bc",
   "metadata": {},
   "source": [
    "## Analysis parameters\n",
    "* `training_features_path`: The path to the training features file which we extracted through previous module `0_Extract_Training_Features.ipynb`.\n",
    "* `reference_map_path`: The path to the reference classification map, which will be used as a stratification layer to extract random samples for each class. In this example, we are using the existing national land cover map in 2016. \n",
    "* `class_attr`: This is the name of column in your shapefile attribute table that contains the class labels. **The class labels must be integers**\n",
    "* `output_crs`: Output spatial reference system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62286a65-b4ae-4939-9c35-8b1675899e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features_path = 'Data/training_features.txt'\n",
    "reference_map_path='Data/reference_classification_map.tif'\n",
    "class_attr = 'LC_Class_I' # class label in integer format\n",
    "output_crs='epsg:32735' # WGS84/UTM Zone 35S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c277e7e2-e195-43b0-9266-77a62813db76",
   "metadata": {},
   "source": [
    "## Load input data\n",
    "\n",
    "We now load the training features .txt file using `pandas`. The pandas dataframe should contain columns `class_attr` identifying class labels and the bi-monthly geomedians of the nine spectral bands and NDVI that we extracted through previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf90a7-4fdf-40a9-90fc-edce613c6f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training features\n",
    "training_features= pd.read_csv(training_features_path)\n",
    "# Plot first five rows\n",
    "training_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1983c2a7",
   "metadata": {},
   "source": [
    "Using the `class_attr` column we can get the class values, which we will use later to process by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5266d539",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_classes=training_features[class_attr].unique() # get class labels\n",
    "print('land cover classes:\\n',lc_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e145b73e",
   "metadata": {},
   "source": [
    "We also retrieve the features from the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad4ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements=list(training_features)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c42d4cf-ba19-45a8-b095-f42a070e4d92",
   "metadata": {},
   "source": [
    "The training data filtering method also requires a reference land cover map as a stratification layer to generate random training samples to train the K-Means models, so We now load the reference map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0486c4df-5048-49ba-a572-90bd8d69d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load reference classification map\n",
    "rf_2017_raster = xr.open_dataset(reference_map_path,engine=\"rasterio\").astype(np.uint8).squeeze(\"band\", drop=True)\n",
    "# # reproject the raster\n",
    "# rf_2017_raster= rf_2017_raster.rio.reproject(resolution=10, dst_crs=crs,resampling=Resampling.nearest)\n",
    "rf_2017_raster=rf_2017_raster.band_data\n",
    "print('Reference land cover classifcation raster:\\n',rf_2017_raster) # note: 255 is nodata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67922b67",
   "metadata": {},
   "source": [
    "## Generating random samples\n",
    "As we would like to cluster the training features, but for some classes there might not be enough training samples to make the K-Means clustering statistically reliable. Therefore, here we generate random point samples for each class using the reference classification map using the `random_sampling` function. This function takes in a few parameters:  \n",
    "* `n`: total number of points to sample\n",
    "* `da`: a classified map as a 2-dimensional xarray.DataArray\n",
    "* `sampling`: the sampling strategy, e.g. 'stratified_random', where each class has a number of points proportional to its relative area, or 'equal_stratified_random' where each class has the same number of points.\n",
    "* `out_fname`: a filepath name for the function to export a shapefile/geojson of the sampling points to file. You can set this to `None` if you don't need to output the file.  \n",
    "\n",
    "The `random_sampling` function will add a `class` attribute to the output points, which identifies the class value of each sample. In this example we are generating ~1000 samples to train the K-Means models, which means we should set n=8000 and use equal stratified random sampling strategy. We also output the samples to a geojson file. To exclude pixel values other than the valid land cover classes (e.g. 0, 255) from the sampling, we can first assign them as NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1f64ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=8000\n",
    "random_samples_path='Results/random_samples.geojson'\n",
    "da=rf_2017_raster.where((rf_2017_raster!=0)&(rf_2017_raster!=255),np.nan)\n",
    "gpd_random_samples=random_sampling(da,n,sampling='equal_stratified_random',out_fname=random_samples_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a07c8c5",
   "metadata": {},
   "source": [
    "## Extract features for the samples\n",
    "\n",
    "With the random sample points available, we can then extract features to train the K-Means models. As we will apply clustering on the training features we extracted in the previous module, we should extract exactly the same features, i.e. bi-monthly geomedian of the nine spectral bands and NDVI as we did in the previous module. Simply re-use the same query and feature layer function in the previous module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bea2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up our inputs to collect_training_data\n",
    "zonal_stats = None\n",
    "# Set up the inputs for the ODC query\n",
    "time = ('2021')\n",
    "# using all spectral bands with 10~20 m spatial resolution\n",
    "measurements = ['blue','green','red','red_edge_1','red_edge_2', 'red_edge_3','nir_1','swir_1','swir_2']\n",
    "resolution = (-10,10)\n",
    "\n",
    "# define query\n",
    "query = {\n",
    "    'time': time,\n",
    "    'measurements': measurements,\n",
    "    'output_crs': output_crs,\n",
    "    'resolution': resolution\n",
    "}\n",
    "\n",
    "# define a function to feature layers\n",
    "def feature_layers(query): \n",
    "    # connect to the datacube so we can access DE Africa data\n",
    "    dc = datacube.Datacube(app='feature_layers')\n",
    "    \n",
    "    # load Sentinel-2 analysis ready data\n",
    "    ds = load_ard(dc=dc,\n",
    "                  products=['s2_l2a'],\n",
    "                  group_by='solar_day',\n",
    "                  verbose=False,\n",
    "                  **query)\n",
    "    \n",
    "    # calculate NDVI\n",
    "    ds = calculate_indices(ds,\n",
    "                           index=['NDVI'],\n",
    "                           drop=False,\n",
    "                           satellite_mission='s2')\n",
    "    \n",
    "    # interpolate nodata using mean of previous and next observation\n",
    "#     ds=ds.interpolate_na(dim='time',method='linear',use_coordinate=False,fill_value='extrapolate')\n",
    "#     ds=ds.interpolate_na(dim='time',method='linear',use_coordinate=False)\n",
    "\n",
    "    # calculate bi-monthly geomedian\n",
    "    ds=ds.resample(time='2MS').map(xr_geomedian)\n",
    "    \n",
    "    # stack multi-temporal measurements and rename them\n",
    "    n_time=ds.dims['time']\n",
    "    list_measurements=list(ds.keys())\n",
    "    list_stack_measures=[]\n",
    "    for j in range(len(list_measurements)):\n",
    "        for k in range(n_time):\n",
    "            variable_name=list_measurements[j]+'_'+str(k)\n",
    "            measure_single=ds[list_measurements[j]].isel(time=k).rename(variable_name)\n",
    "            list_stack_measures.append(measure_single)\n",
    "    ds_stacked=xr.merge(list_stack_measures,compat='override')\n",
    "    return ds_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c0fb7",
   "metadata": {},
   "source": [
    "Now we can extract the training features for the random sample points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f57cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect the number of CPUs\n",
    "ncpus=round(get_cpu_quota())\n",
    "print('ncpus = '+str(ncpus))\n",
    "\n",
    "# collect training data\n",
    "column_names, model_input = collect_training_data(\n",
    "    gdf=gpd_random_samples[0:10], # replace with gdf=training_points if you are extracting all the training data\n",
    "    dc_query=query,\n",
    "    ncpus=ncpus,\n",
    "    field='class',\n",
    "    zonal_stats=None,\n",
    "    feature_func=feature_layers,\n",
    "    return_coords=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50021c25-ec7c-477f-8833-31bf6a44927a",
   "metadata": {},
   "source": [
    "## K-Means clustering and filtering\n",
    "Now that we have the features of random samples and training points, we can use them to train and apply the K-Means models. The K-Means model requires a pre-defined number of clusters, which is unknown for our case and varied depending on the distribution of the samples. One way to identify the optimal number of clusters is using the Calinski and Harabasz score. The score is the ratio of the sum of between-clusters dispersion and of within-cluster dispersion for all clusters, where the score is higher when clusters are dense and well separated. More information about the score can be checked [here](https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index). Here we calculate the scores of varied number of clusters and retain the model with highest score.\n",
    "\n",
    "Note that K-Means model is sensitive to feature scales, so we need to standardise all features before applying the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312882ba-f3b2-4255-b75d-cb38e1e0af5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() # standard scaler for input data standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2f7c96-6bbb-4670-a300-24bf389d72bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpus=round(get_cpu_quota())\n",
    "print('ncpus = '+str(ncpus))\n",
    "tiles_shp='Data/Mozambique_tiles_biggest1.shp'\n",
    "\n",
    "# get bounding boxes of tiles\n",
    "tiles=gpd.read_file(tiles_shp).to_crs(crs)\n",
    "tile_bboxes=tiles.bounds\n",
    "print('tile boundaries for Mozambique: \\n',tile_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd905a2-2137-4fb0-8c4b-3098c8c96b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "frequency_threshold=0.05 # threshold of cluter frequency\n",
    "td2021_filtered=None # filtered training data\n",
    "# filtering training data for each class\n",
    "# for i in lc_classes[8:]:\n",
    "for i in lc_classes:\n",
    "    #i=1 # test for first class\n",
    "    print('Processing class ',i)\n",
    "    gpd_samples=None\n",
    "    n_total=np.sum(rf_2017_raster.to_numpy()==i)\n",
    "    # generate randomly sampled data to fit and optimise a kmeans clusterer\n",
    "    for n in range(len(tile_bboxes)):\n",
    "        print('stratified random sampling from tile ',n)\n",
    "        da_mask=rf_2017_raster.rio.clip([tiles.iloc[n].geometry],crs=crs,drop=True)\n",
    "        da_mask=da_mask.rio.reproject(dst_crs=crs,resampling=Resampling.nearest)\n",
    "        n_samples_tile=n_samples*np.sum(da_mask.to_numpy()==i)/n_total\n",
    "        gpd_samples_tile=random_sampling(da_mask,n_samples_tile,sampling='manual',\n",
    "                                         manual_class_ratios={str(i):n_samples_tile},out_fname=None)\n",
    "        if gpd_samples is None:\n",
    "            gpd_samples=gpd_samples_tile\n",
    "        else:\n",
    "            gpd_samples=pd.concat([gpd_samples,gpd_samples_tile])\n",
    "    # get data array\n",
    "#     da_mask=da_mask.where(da_mask==i,np.nan) # replace other class values as nan so they won't be sampled (comment due to large memory required)\n",
    "#     gpd_samples=random_sampling(da_mask,n_samples,sampling='stratified_random',manual_class_ratios=None,out_fname=None)\n",
    "#     gpd_samples=random_sampling(da_mask,n_samples,sampling='manual',manual_class_ratios={str(i):n_samples},out_fname=None)\n",
    "    gpd_samples=gpd_samples.reset_index(drop=True).drop(columns=['spatial_ref','class']) # drop this attribute derived from random_sampling function\n",
    "    gpd_samples[class_attr]=i # add attribute field so that we can use collect_training_data function\n",
    "    if gpd_samples.crs is None:\n",
    "        gpd_samples=gpd_samples.set_crs(crs)\n",
    "    print('radomly sampled points for class ',i,'\\n',gpd_samples)\n",
    "    # extract data for the random samples\n",
    "    column_names, sampled_data = collect_training_data(gdf=gpd_samples,\n",
    "                                                          dc_query=query,\n",
    "                                                          ncpus=ncpus,\n",
    "#                                                           ncpus=1,\n",
    "                                                          field=class_attr, \n",
    "                                                          zonal_stats=zonal_stats,\n",
    "                                                          feature_func=feature_layers,\n",
    "                                                          return_coords=False)\n",
    "    # standardise features\n",
    "    scaler=scaler.fit(sampled_data[:,1:])\n",
    "    sampled_data=scaler.transform(sampled_data[:,1:])\n",
    "#     sampled_data[:,-6:]=sampled_data[:,-6:]*10000\n",
    "#     sampled_data=sampled_data[:,1:]\n",
    "    # fit kmeans model using the sample training data\n",
    "    # first find optimal number of clusters based on Calinski-Harabasz index\n",
    "    highest_score=-999\n",
    "    n_cluster_optimal=5\n",
    "    kmeans_model_optimal=None # initialise optimal model parameters\n",
    "    labels_optimal=None\n",
    "    for n_cluster in range(5,26):\n",
    "        kmeans_model = KMeans(n_clusters=n_cluster, random_state=1).fit(sampled_data)\n",
    "        labels=kmeans_model.predict(sampled_data)\n",
    "        score=metrics.calinski_harabasz_score(sampled_data, labels)\n",
    "#         score=metrics.davies_bouldin_score(sampled_data, labels)\n",
    "        print('Calinski-Harabasz score for ',n_cluster,' clusters is: ',score)\n",
    "#         print('Davies-Bouldin score for ',n_cluster,' clusters is: ',score)\n",
    "        if (highest_score==-999)or(highest_score<score):\n",
    "#         if (highest_score==-999)or(highest_score>score):\n",
    "            highest_score=score\n",
    "            n_cluster_optimal=n_cluster\n",
    "            kmeans_model_optimal=kmeans_model\n",
    "            labels_optimal=labels\n",
    "    print('Best number of clusters for class %s: %s'%(i,n_cluster_optimal))\n",
    "    \n",
    "    # subset original training points for this class\n",
    "    td_single_class=training_features[training_features[class_attr]==i].reset_index(drop=True)\n",
    "    print('Number of training data collected: ',len(td_single_class))\n",
    "    column_names, model_input = collect_training_data(gdf=td_single_class,\n",
    "                                                      dc_query=query,\n",
    "                                                      ncpus=ncpus,\n",
    "                                                      field=class_attr,\n",
    "                                                      zonal_stats=zonal_stats,\n",
    "                                                      feature_func=feature_layers,\n",
    "                                                      return_coords=True)\n",
    "    print('Number of training data after removing Nans and Infs: ',model_input.shape[0])\n",
    "    # first covert the training data to pandas\n",
    "    td_single_class_filtered=pd.DataFrame(data=model_input,columns=column_names)\n",
    "    # then to geopandas dataframe\n",
    "    td_single_class_filtered=gpd.GeoDataFrame(td_single_class_filtered, \n",
    "                                    geometry=gpd.points_from_xy(model_input[:,-2], model_input[:,-1],\n",
    "                                                                crs=crs))\n",
    "    # normalisation before clustering\n",
    "    model_input=scaler.transform(model_input[:,1:-2])\n",
    "#     model_input=model_input[:,1:-2]\n",
    "#     model_input[:,-6:]=model_input[:,-6:]*10000\n",
    "    # predict clustering labels\n",
    "    labels_kmeans = kmeans_model_optimal.predict(model_input)\n",
    "    # append clustering results to pixel coordinates\n",
    "    td_single_class_filtered['cluster']=labels_kmeans\n",
    "    # append frequency of each cluster\n",
    "    labels_optimal=pd.DataFrame(data=labels_optimal,columns=['cluster']) # calculate cluster frequencies of the random samples\n",
    "    cluster_frequency=td_single_class_filtered['cluster'].map(labels_optimal['cluster'].value_counts(normalize=True))\n",
    "    td_single_class_filtered['cluster_frequency']=cluster_frequency\n",
    "#     print('filtered training data: \\n',td_single_class_filtered[td_single_class_filtered['cluster_frequency']<frequency_threshold])\n",
    "    # filter by cluster frequency\n",
    "    td_single_class_filtered=td_single_class_filtered[td_single_class_filtered['cluster_frequency']>=frequency_threshold]\n",
    "    print('Number of training data after filtering: ',len(td_single_class_filtered))\n",
    "    # export filtered training data for this class as shapefile (will encounter 10-character limit for attributes)\n",
    "#     td_single_class_filtered.to_file('Results/landcover_td2021_filtered_DEAfrica_new_class_'+str(i)+'.shp')\n",
    "    # export filtered training data for this class as geojson file\n",
    "    td_single_class_filtered.to_file('Results/landcover_td2021_filtered_class_'+str(i)+'.geojson', driver=\"GeoJSON\")\n",
    "    # append the filtered training points of this class to final filtered training data\n",
    "    if td2021_filtered is None:\n",
    "        td2021_filtered=td_single_class_filtered\n",
    "    else:\n",
    "        td2021_filtered=pd.concat([td2021_filtered, td_single_class_filtered])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffedd3d-6817-4c3a-aca6-d5b5d98f85ad",
   "metadata": {},
   "source": [
    "## Export filtered training features\n",
    "Once we've filtered the training signatures, we can write the filtered data to disk. The full filtered training features file is provided as 'Results/landcover_td2021_filtered.txt', which will allow us to import the data in the next step(s) of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b97829f-3578-4ce3-95e6-ba2be2b99820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training data for all classes\n",
    "print('filtered training data for 2021:\\n',td2021_filtered)\n",
    "td2021_filtered.to_file('Results/landcover_td2021_filtered.geojson', driver=\"GeoJSON\")\n",
    "\n",
    "# export the filtered training data as txt file\n",
    "output_file = \"Results/landcover_td2021_filtered.txt\"\n",
    "td2021_filtered.to_csv(output_file, header=True, index=None, sep=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('geoenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "01a9736267bf300689e8e016092cd01f0c67384dd94651ae6e139a291bc8cc97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
