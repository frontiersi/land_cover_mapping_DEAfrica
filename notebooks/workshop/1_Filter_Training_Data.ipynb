{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4555d86-4eb4-4185-93d8-52f55da43df9",
   "metadata": {},
   "source": [
    "# Filter Training Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f767b5b-dd6e-48f8-aba4-259d72d8804a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Background\n",
    "\n",
    "It is not uncommon that existing training data were collected at different time period than the study period. This means the dataset may not reflect the real ground cover due to temporal changes. FAO adopted a training data filtering method for any given reference year that is within a time span (e.g. 5 years) from an existing baseline, and tested the method in the production of land cover mapping for Lesotho. It is assumed that the majority of reference labels will remain valid from one year to the previous/next. Based on this assumption, the reference labels which have changed are the minority, and should be detectable through the use of outlier detection methods like K-Means clustering. More details on the method and how it works for Lesotho can be found in the published paper ([De Simone et al 2022](https://www.mdpi.com/2072-4292/14/14/3294))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ec80f-8f94-45f9-808c-712a572f8d89",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "This notebook will implement automatic filtering of a training dataset for a target year using points from a geojson or shapefile and a reference map of a previous year. The steps include:\n",
    "1. Load extracted training features\n",
    "2. Generate stratified random samples for each class and extract their features using `random_sampling` and `collect_training_data`\n",
    "3. Train K-Means models using the features of the random samples\n",
    "4. Apply clustering on training features and remove minor clusters, i.e. cluster size smaller than 5% of overall sample size\n",
    "5. Export the filtered training data to disk for use in subsequent scripts\n",
    "\n",
    "To run this analysis, run all the cells in the notebook, starting with the \"Load packages\" cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdba3177-6db9-4440-9c8b-d5437f41e7f4",
   "metadata": {},
   "source": [
    "### Load packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fccc7e0-768c-4479-a777-6ce9efbf4d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import datacube\n",
    "import warnings\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "from odc.io.cgroups import get_cpu_quota\n",
    "from odc.algo import xr_geomedian\n",
    "from deafrica_tools.datahandling import load_ard\n",
    "from deafrica_tools.bandindices import calculate_indices\n",
    "from deafrica_tools.classification import collect_training_data\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from rasterio.enums import Resampling\n",
    "from random_sampling import random_sampling # adapted from function by Chad Burton: https://gist.github.com/cbur24/04760d645aa123a3b1817b07786e7d9f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efee6d6-c909-427c-bcb7-2943ae33b6bc",
   "metadata": {},
   "source": [
    "## Analysis parameters\n",
    "* `training_features_path`: The path to the training features file which we extracted through previous step. A default geojson is provided for this notebook.\n",
    "* `reference_map_path`: The path to the reference classification map, which will be used as a stratification layer for extracting random samples for each class. In this example, we are using the existing national land cover map in 2016. \n",
    "* `class_attr`: This is the name of column in your shapefile attribute table that contains the class labels. **The class labels must be integers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62286a65-b4ae-4939-9c35-8b1675899e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths and attributes\n",
    "training_features_path = 'Data/trainning_samples_FNDS_II_SOM_2016.geojson'\n",
    "reference_map_path='Data/Landcover_map_ODC_Brazil_2015_2016.tif'\n",
    "class_attr = 'LC_Class_I' # class label in integer format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c277e7e2-e195-43b0-9266-77a62813db76",
   "metadata": {},
   "source": [
    "## Load input data\n",
    "\n",
    "We now load the training features geojson file using `geopandas`. The geopandas dataframe should contain a 'class_attr' column identifying class labels, and the bi-monthly geomedians of the nine spectral bands and NDVI that we extracted through previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf90a7-4fdf-40a9-90fc-edce613c6f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference land cover survey points and reproject\n",
    "training_data2017= gpd.read_file(training_features_path).to_crs(crs) # read training points as geopandas dataframe\n",
    "training_data2017=training_data2017[[class_attr,'geometry']] # select attributes\n",
    "print('land cover survey points 2017:\\n',training_data2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c42d4cf-ba19-45a8-b095-f42a070e4d92",
   "metadata": {},
   "source": [
    "The training data filtering method also requires a baseline land cover map as a stratification layer to produce random training samples to train the K-Means model. We now load the raster map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0486c4df-5048-49ba-a572-90bd8d69d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load initial classification map\n",
    "rf_2017_raster = xr.open_dataset(reference_map_path,engine=\"rasterio\").astype(np.uint8).squeeze(\"band\", drop=True)\n",
    "# # reproject the raster\n",
    "# rf_2017_raster= rf_2017_raster.rio.reproject(resolution=10, dst_crs=crs,resampling=Resampling.nearest)\n",
    "rf_2017_raster=rf_2017_raster.band_data\n",
    "print('Reference land cover classifcation raster:\\n',rf_2017_raster) # note: 255 is nodata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e2652c-bf91-4d80-9d66-0537dbe30018",
   "metadata": {},
   "source": [
    "## Defining feature function\n",
    "We now define the same query and feature layer function that we used to extract training features in the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff1b5f8-3028-49a2-88cc-a136fc693d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "land cover classes:\n",
      " [3 5 1 2 4]\n"
     ]
    }
   ],
   "source": [
    "zonal_stats = None\n",
    "measurements = ['blue','green','red','red_edge_1','red_edge_2', 'red_edge_3','nir_1','nir_2','swir_1','swir_2']\n",
    "query = {\n",
    "    'time': ('2021-01', '2021-12'),\n",
    "    'measurements': measurements,\n",
    "    'output_crs': crs,\n",
    "    'resolution': (-10, 10)\n",
    "}\n",
    "# define a function to feature layers\n",
    "def feature_layers(query): \n",
    "    #connect to the datacube\n",
    "    dc = datacube.Datacube(app='feature_layers')\n",
    "    ds = load_ard(dc=dc,\n",
    "                  products=['s2_l2a'],\n",
    "                  group_by='solar_day',\n",
    "                  verbose=False,\n",
    "#                   mask_filters=[(\"opening\", 2)], # morphological opening by 2 pixels to remove small masked regions\n",
    "                  **query)\n",
    "    ds = calculate_indices(ds,\n",
    "                           index=['NDVI'],\n",
    "                           drop=False,\n",
    "                           satellite_mission='s2')\n",
    "    # interpolate nodata using mean of previous and next observation\n",
    "#     ds=ds.interpolate_na(dim='time',method='linear',use_coordinate=False,fill_value='extrapolate')\n",
    "#     ds=ds.interpolate_na(dim='time',method='linear',use_coordinate=False)\n",
    "    # calculate geomedians within each two-month interval\n",
    "    ds=ds.resample(time='2MS').map(xr_geomedian)\n",
    "    # stack multi-temporal measurements and rename them\n",
    "    n_time=ds.dims['time']\n",
    "    list_measurements=list(ds.keys())\n",
    "    ds_stacked=None\n",
    "    for j in range(len(list_measurements)):\n",
    "        for k in range(n_time):\n",
    "            variable_name=list_measurements[j]+'_'+str(k)\n",
    "            # print ('Stacking band ',list_measurements[j],' at time ',k)\n",
    "            measure_single=ds[list_measurements[j]].isel(time=k).rename(variable_name)\n",
    "            if ds_stacked is None:\n",
    "                ds_stacked=measure_single\n",
    "            else:\n",
    "                ds_stacked=xr.merge([ds_stacked,measure_single],compat='override')\n",
    "    return ds_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50021c25-ec7c-477f-8833-31bf6a44927a",
   "metadata": {},
   "source": [
    "## K-Means clustering\n",
    "We first extract random samples using the reference layer. Then we apply clustering on a per-class basis. To allow for iteration through each class, we first get the class labels from the training features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312882ba-f3b2-4255-b75d-cb38e1e0af5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_classes=training_data2017[class_attr].unique() # get class labels\n",
    "print('land cover classes:\\n',lc_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4209b11-8fdf-44bd-91f6-6dea4704c817",
   "metadata": {},
   "source": [
    "Now we can start the clustering by class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2f7c96-6bbb-4670-a300-24bf389d72bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpus=round(get_cpu_quota())\n",
    "print('ncpus = '+str(ncpus))\n",
    "tiles_shp='Data/Mozambique_tiles_biggest1.shp'\n",
    "crs='epsg:32736' # WGS84/UTM Zone 36S\n",
    "\n",
    "# get bounding boxes of tiles\n",
    "tiles=gpd.read_file(tiles_shp).to_crs(crs)\n",
    "tile_bboxes=tiles.bounds\n",
    "print('tile boundaries for Mozambique: \\n',tile_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd905a2-2137-4fb0-8c4b-3098c8c96b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples=1000 # number of random samples to optimise number of clusters for kmeans\n",
    "scaler = StandardScaler() # standard scaler for input data standardisation\n",
    "frequency_threshold=0.05 # threshold of cluter frequency\n",
    "td2021_filtered=None # filtered training data\n",
    "# filtering training data for each class\n",
    "# for i in lc_classes[8:]:\n",
    "for i in lc_classes:\n",
    "    #i=1 # test for first class\n",
    "    print('Processing class ',i)\n",
    "    gpd_samples=None\n",
    "    n_total=np.sum(rf_2017_raster.to_numpy()==i)\n",
    "    # generate randomly sampled data to fit and optimise a kmeans clusterer\n",
    "    for n in range(len(tile_bboxes)):\n",
    "        print('stratified random sampling from tile ',n)\n",
    "        da_mask=rf_2017_raster.rio.clip([tiles.iloc[n].geometry],crs=crs,drop=True)\n",
    "        da_mask=da_mask.rio.reproject(dst_crs=crs,resampling=Resampling.nearest)\n",
    "        n_samples_tile=n_samples*np.sum(da_mask.to_numpy()==i)/n_total\n",
    "        gpd_samples_tile=random_sampling(da_mask,n_samples_tile,sampling='manual',\n",
    "                                         manual_class_ratios={str(i):n_samples_tile},out_fname=None)\n",
    "        if gpd_samples is None:\n",
    "            gpd_samples=gpd_samples_tile\n",
    "        else:\n",
    "            gpd_samples=pd.concat([gpd_samples,gpd_samples_tile])\n",
    "    # get data array\n",
    "#     da_mask=da_mask.where(da_mask==i,np.nan) # replace other class values as nan so they won't be sampled (comment due to large memory required)\n",
    "#     gpd_samples=random_sampling(da_mask,n_samples,sampling='stratified_random',manual_class_ratios=None,out_fname=None)\n",
    "#     gpd_samples=random_sampling(da_mask,n_samples,sampling='manual',manual_class_ratios={str(i):n_samples},out_fname=None)\n",
    "    gpd_samples=gpd_samples.reset_index(drop=True).drop(columns=['spatial_ref','class']) # drop this attribute derived from random_sampling function\n",
    "    gpd_samples[class_attr]=i # add attribute field so that we can use collect_training_data function\n",
    "    if gpd_samples.crs is None:\n",
    "        gpd_samples=gpd_samples.set_crs(crs)\n",
    "    print('radomly sampled points for class ',i,'\\n',gpd_samples)\n",
    "    # extract data for the random samples\n",
    "    column_names, sampled_data = collect_training_data(gdf=gpd_samples,\n",
    "                                                          dc_query=query,\n",
    "                                                          ncpus=ncpus,\n",
    "#                                                           ncpus=1,\n",
    "                                                          field=class_attr, \n",
    "                                                          zonal_stats=zonal_stats,\n",
    "                                                          feature_func=feature_layers,\n",
    "                                                          return_coords=False)\n",
    "    # standardise features\n",
    "    scaler=scaler.fit(sampled_data[:,1:])\n",
    "    sampled_data=scaler.transform(sampled_data[:,1:])\n",
    "#     sampled_data[:,-6:]=sampled_data[:,-6:]*10000\n",
    "#     sampled_data=sampled_data[:,1:]\n",
    "    # fit kmeans model using the sample training data\n",
    "    # first find optimal number of clusters based on Calinski-Harabasz index\n",
    "    highest_score=-999\n",
    "    n_cluster_optimal=5\n",
    "    kmeans_model_optimal=None # initialise optimal model parameters\n",
    "    labels_optimal=None\n",
    "    for n_cluster in range(5,26):\n",
    "        kmeans_model = KMeans(n_clusters=n_cluster, random_state=1).fit(sampled_data)\n",
    "        labels=kmeans_model.predict(sampled_data)\n",
    "        score=metrics.calinski_harabasz_score(sampled_data, labels)\n",
    "#         score=metrics.davies_bouldin_score(sampled_data, labels)\n",
    "        print('Calinski-Harabasz score for ',n_cluster,' clusters is: ',score)\n",
    "#         print('Davies-Bouldin score for ',n_cluster,' clusters is: ',score)\n",
    "        if (highest_score==-999)or(highest_score<score):\n",
    "#         if (highest_score==-999)or(highest_score>score):\n",
    "            highest_score=score\n",
    "            n_cluster_optimal=n_cluster\n",
    "            kmeans_model_optimal=kmeans_model\n",
    "            labels_optimal=labels\n",
    "    print('Best number of clusters for class %s: %s'%(i,n_cluster_optimal))\n",
    "    \n",
    "    # subset original training points for this class\n",
    "    td_single_class=training_data2017[training_data2017[class_attr]==i].reset_index(drop=True)\n",
    "    print('Number of training data collected: ',len(td_single_class))\n",
    "    column_names, model_input = collect_training_data(gdf=td_single_class,\n",
    "                                                      dc_query=query,\n",
    "                                                      ncpus=ncpus,\n",
    "                                                      field=class_attr,\n",
    "                                                      zonal_stats=zonal_stats,\n",
    "                                                      feature_func=feature_layers,\n",
    "                                                      return_coords=True)\n",
    "    print('Number of training data after removing Nans and Infs: ',model_input.shape[0])\n",
    "    # first covert the training data to pandas\n",
    "    td_single_class_filtered=pd.DataFrame(data=model_input,columns=column_names)\n",
    "    # then to geopandas dataframe\n",
    "    td_single_class_filtered=gpd.GeoDataFrame(td_single_class_filtered, \n",
    "                                    geometry=gpd.points_from_xy(model_input[:,-2], model_input[:,-1],\n",
    "                                                                crs=crs))\n",
    "    # normalisation before clustering\n",
    "    model_input=scaler.transform(model_input[:,1:-2])\n",
    "#     model_input=model_input[:,1:-2]\n",
    "#     model_input[:,-6:]=model_input[:,-6:]*10000\n",
    "    # predict clustering labels\n",
    "    labels_kmeans = kmeans_model_optimal.predict(model_input)\n",
    "    # append clustering results to pixel coordinates\n",
    "    td_single_class_filtered['cluster']=labels_kmeans\n",
    "    # append frequency of each cluster\n",
    "    labels_optimal=pd.DataFrame(data=labels_optimal,columns=['cluster']) # calculate cluster frequencies of the random samples\n",
    "    cluster_frequency=td_single_class_filtered['cluster'].map(labels_optimal['cluster'].value_counts(normalize=True))\n",
    "    td_single_class_filtered['cluster_frequency']=cluster_frequency\n",
    "#     print('filtered training data: \\n',td_single_class_filtered[td_single_class_filtered['cluster_frequency']<frequency_threshold])\n",
    "    # filter by cluster frequency\n",
    "    td_single_class_filtered=td_single_class_filtered[td_single_class_filtered['cluster_frequency']>=frequency_threshold]\n",
    "    print('Number of training data after filtering: ',len(td_single_class_filtered))\n",
    "    # export filtered training data for this class as shapefile (will encounter 10-character limit for attributes)\n",
    "#     td_single_class_filtered.to_file('Results/landcover_td2021_filtered_DEAfrica_new_class_'+str(i)+'.shp')\n",
    "    # export filtered training data for this class as geojson file\n",
    "    td_single_class_filtered.to_file('Results/landcover_td2021_filtered_class_'+str(i)+'.geojson', driver=\"GeoJSON\")\n",
    "    # append the filtered training points of this class to final filtered training data\n",
    "    if td2021_filtered is None:\n",
    "        td2021_filtered=td_single_class_filtered\n",
    "    else:\n",
    "        td2021_filtered=pd.concat([td2021_filtered, td_single_class_filtered])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffedd3d-6817-4c3a-aca6-d5b5d98f85ad",
   "metadata": {},
   "source": [
    "## Export filtered training features\n",
    "Once we've filtered the training signatures, we can write the filtered data to disk. The full filtered training features file is provided as 'Results/landcover_td2021_filtered.txt', which will allow us to import the data in the next step(s) of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b97829f-3578-4ce3-95e6-ba2be2b99820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training data for all classes\n",
    "print('filtered training data for 2021:\\n',td2021_filtered)\n",
    "td2021_filtered.to_file('Results/landcover_td2021_filtered.geojson', driver=\"GeoJSON\")\n",
    "\n",
    "# export the filtered training data as txt file\n",
    "output_file = \"Results/landcover_td2021_filtered.txt\"\n",
    "td2021_filtered.to_csv(output_file, header=True, index=None, sep=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
